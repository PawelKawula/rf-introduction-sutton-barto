{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce527a47-efcd-4795-a7ec-e658c2b4779d",
   "metadata": {},
   "source": [
    "### 1\n",
    "We have not explicitly considered or given pseudocode for any Monte Carlo\n",
    "methods in this chapter. What would they be like? Why is it reasonable not to give\n",
    "pseudocode for them? How would they perform on the Mountain Car task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455cad72-1ad4-4d35-9906-266ac0a43fc5",
   "metadata": {},
   "source": [
    "They will be very similar to these defined in chapter 9, so there is no need to repeat.\n",
    "They would perform badly since we would not try every action first since we would\n",
    "not change value function until we finish the episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e04ef-48f2-487e-b784-ecef6d75bb5b",
   "metadata": {},
   "source": [
    "### 2\n",
    "Give pseudocode for semi-gradient one-step Expected Sarsa for control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae1fcb0-8f0c-425c-8c1c-f31c6e0e1897",
   "metadata": {},
   "source": [
    "Input: differentiable action-value function $bar q: S x A x \\mathbb{R}^d -> \\mathbb{R}$  \n",
    "Input: a policy $\\pi$ (if estimating $q_\\pi$)  \n",
    "Algorithm parameters: step size $\\alpha > 0$, small $\\epsilon > 0$  \n",
    "Initialize value-function weights $w \\in \\mathbb{R}^d$ arbitrarly (e.g., $w = 0$)  \n",
    "Loop for each episode:  \n",
    "&emsp;Initialize state and action $S_0, A_0 \\approx \\pi(\\cdot|S_0)$ or $\\epsilon$-greedy wrt $\\bar q(S_0, \\cdot, w)$:  \n",
    "&emsp;while $S \\neq terminal$  \n",
    "&emsp;&emsp;Get next state $S'$ and reward $R$  \n",
    "&emsp;&emsp;$\\bar v(S, A, w) = \\bar v(S, A, w) + \\alpha(R + \\gamma \\sum_{a} \\pi(a|S')\\bar v(S', a, w) - \\bar v(S, a, w))$  \n",
    "&emsp;&emsp;$S = S'$  \n",
    "&emsp;&emsp;$A \\approx \\pi(s)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38507bc-64cb-4bf9-9de3-d95a9b78bfea",
   "metadata": {},
   "source": [
    "### 3\n",
    "Why do the results shown in Figure 10.4 have higher standard errors at\n",
    "large n than at small n?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aaaea6-fc80-457c-b7b1-10447f99ee84",
   "metadata": {},
   "source": [
    "That's because value function was changing more drastically for large n, and\n",
    "because for most of the time reward was 0 they were shaped mostly by later timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2ec1b-d538-4293-9c4d-23159ceaa11f",
   "metadata": {},
   "source": [
    "### 4\n",
    "Give pseudocode for a differential version of semi-gradient Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcca069-e32d-4729-82c9-c40eb6646370",
   "metadata": {},
   "source": [
    "Input: a differentiable action-value function parametrization $\\bar q:$\n",
    "$\\mathbb{S} x \\mathbb{A} x \\mathbb{R}^d \\rightarrow \\mathbb{R}$  \n",
    "Algorithm parameters: step sizes $\\alpha, \\beta > 0$, small $\\epsilon > 0$  \n",
    "Initialize value-function weights $w \\in R^d$ arbitrarly (e.g., $w = 0$)\n",
    "Initialize average reward-estimate $\\bar R$ = arbitrarly (e.g., $\\bar R = 0$\n",
    "  \n",
    "Initialize state $S$ and action $A$  \n",
    "Loop for each step:  \n",
    "&emsp;Take action $A$, observe $R, S'$  \n",
    "&emsp;$\\delta = R - \\bar R + \\bar {max}_a q(S', a, w) - q(S', A, w)$  \n",
    "&emsp;$\\bar R = \\bar R + \\beta\\delta$  \n",
    "&emsp;$\\bar q(S, A, w) = \\bar q(S, A, w) + \\alpha\\delta\\nabla\\bar q(S, A, w)$  \n",
    "&emsp;$A \\approx\\pi (S');$  \n",
    "&emsp;$S = S'$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaef99c-b202-4e25-b1ff-ea2c57c4dc0c",
   "metadata": {},
   "source": [
    "### 5\n",
    "What equations are needed (beyond 10.10) to specify the differential\n",
    "version of TD(0)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ae9c7-a983-4765-9ad2-01bc2bf2077a",
   "metadata": {},
   "source": [
    "10.12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a9dedb-0088-444d-b92a-1d040e61d979",
   "metadata": {},
   "source": [
    "### 6\n",
    "Suppose there is an MDP that under any policy produces the deterministic\n",
    "sequence of rewards +1, 0, +1, 0, +1, 0, . . . going on forever. Technically, this violates\n",
    "ergodicity; there is no stationary limiting distribution $µ_\\pi$ and the limit (10.7) does not\n",
    "exist. Nevertheless, the average reward (10.6) is well defined. What is it? Now consider\n",
    "two states in this MDP. From A, the reward sequence is exactly as described above,\n",
    "starting with a +1, whereas, from B, the reward sequence starts with a 0 and then\n",
    "continues with +1, 0, +1, 0, . . .. We would like to compute the differential values of A and\n",
    "B. Unfortunately, the differential return (10.9) is not well defined when starting from\n",
    "these states as the implicit limit does not exist. To repair this, one could alternatively\n",
    "define the differential value of a state as  \n",
    "$\\mathbb{v}_\\pi(s) = \\lim_{\\gamma\\to1}\\lim_{h\\to\\infty}\\sum_{t=0}^h\\gamma^t\\left(\\mathbb{E}_\\pi [R_{t+1}|S_0=s]-r(\\pi)\\right)$  \n",
    "Under this definition, what are the differential values of states A and B?\n",
    "(10.13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f63f482-4032-4610-9b26-37f81bb3ecd0",
   "metadata": {},
   "source": [
    "For A:  \n",
    "$\\sum_{k=0}^\\infty (\\gamma^{2k} 0.5 - \\gamma^{2k+1} 0.5)$  \n",
    "&emsp;$=\\sum_{k=0}^\\infty [\\gamma^{2k}0.5(1-\\gamma)]$  \n",
    "&emsp;$=\\frac{0.5(1-\\gamma)}{1-\\gamma^2} = \\frac{0.5}{1+\\gamma}$  \n",
    "For B:  \n",
    "$\\sum_{k=0}^\\infty (\\gamma^{2k+1} 0.5 - \\gamma^{2k} 0.5)$  \n",
    "&emsp;$=\\sum_{k=0}^\\infty [\\gamma^{2k}0.5(\\gamma-1)]$  \n",
    "&emsp;$=\\frac{0.5(\\gamma-1)}{1-\\gamma^2} = -\\frac{0.5}{1+\\gamma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce677ead-6802-4d05-8d1d-b21b788bb0da",
   "metadata": {},
   "source": [
    "### 7\n",
    "Consider a Markov reward process consisting of a ring of three states A, B,\n",
    "and C, with state transitions going deterministically around the ring. A reward of +1 is\n",
    "received upon arrival in A and otherwise the reward is 0. What are the differential values\n",
    "of the three states, using (10.13)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2f52a-6eb6-4aab-a645-e67d408f9404",
   "metadata": {},
   "source": [
    "$\\color{red}{v(A) = 0 - \\frac13 + \\gamma0 - \\gamma\\frac13 + \\gamma^2 \\frac13 -\\gamma^2 \\frac13= -\\frac13}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4637b6-598d-40b0-bc9a-7aede401ee8f",
   "metadata": {},
   "source": [
    "$v(A) = \\lim_{\\gamma \\rightarrow1}\\lim_{h \\rightarrow\\infty}\\sum_{t=0}^h\\gamma^t(\\mathbb{E}_\\pi[R_{t+1}|S_0=s] - r(\\pi))$  \n",
    "&emsp;$=\\lim_{\\gamma \\rightarrow1}\\lim_{h \\rightarrow\\infty}(\\sum_{t=0}^h(-\\gamma^{3t}\\frac13) +$\n",
    "$\\gamma\\sum_{t=0}^h(-\\gamma^{3t}\\frac13) + \\gamma^2\\sum_{t=0}^h(\\gamma^{3t}\\frac23))$  \n",
    "&emsp;$=\\lim_{\\gamma \\rightarrow1}\\lim_{h \\rightarrow\\infty}(-\\frac13 - \\gamma\\frac13+\\gamma^2\\frac23)$\n",
    "$\\frac{1-\\gamma^{3h}}{1-\\gamma^3}$  \n",
    "&emsp;$=\\lim_{\\gamma \\rightarrow1}(-\\frac13 - \\gamma\\frac13+\\gamma^2\\frac23)\\frac1{1-\\gamma^3}$\n",
    "&emsp;$=\\lim_{\\gamma \\rightarrow1}(-\\frac13 - \\gamma\\frac13+\\gamma^2\\frac23)\\frac1{(1-\\gamma)(1 + \\gamma + \\gamma^2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86460634-f22f-4585-bfbf-962fdfd2a7ee",
   "metadata": {},
   "source": [
    "$v(B) = \\sum_{t=0}^\\infty\\gamma^t(0 - \\frac13)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53339ffd-a32d-479b-aa2c-9920fe8249b4",
   "metadata": {},
   "source": [
    "$v(C) = \\sum_{t=0}^\\infty\\gamma^t(\\frac13 - \\frac13) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0ce8a-7927-45bc-8b0b-5c34bc18bb14",
   "metadata": {},
   "source": [
    "### 8\n",
    "The pseudocode in the box on page 251 updates $R̄_t$ using $t$ as an error\n",
    "rather than simply $R_{t+1} - \\bar R_t$ . Both errors work, but using $\\delta_t$ is better. To see why,\n",
    "consider the ring MRP of three states from Exercise 10.7. The estimate of the average\n",
    "reward should tend towards its true value of $\\frac13$ . Suppose it was already there and was\n",
    "held stuck there. What would the sequence of $R_{t+1} - R̄_t$ errors be? What would the\n",
    "sequence of $\\delta_t$ errors be (using Equation 10.10)? Which error sequence would produce\n",
    "a more stable estimate of the average reward if the estimate were allowed to change in\n",
    "response to the errors? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a6834-6180-4dcc-bf2d-a3daa972c1d3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b63a81ec-40f8-4766-8ff9-4c66a415e629",
   "metadata": {},
   "source": [
    "### 9\n",
    "In the differential semi-gradient n-step Sarsa algorithm, the step-size\n",
    "parameter on the average reward, , needs to be quite small so that R̄ becomes a good\n",
    "long-term estimate of the average reward. Unfortunately, R̄ will then be biased by its\n",
    "initial value for many steps, which may make learning inefficient. Alternatively, one could\n",
    "use a sample average of the observed rewards for R̄. That would initially adapt rapidly\n",
    "but in the long run would also adapt slowly. As the policy slowly changed, R̄ would also\n",
    "change; the potential for such long-term nonstationarity makes sample-average methods\n",
    "ill-suited. In fact, the step-size parameter on the average reward is a perfect place to use\n",
    "the unbiased constant-step-size trick from Exercise 2.7. Describe the specific changes\n",
    "needed to the boxed algorithm for differential semi-gradient n-step Sarsa to use this\n",
    "trick."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
