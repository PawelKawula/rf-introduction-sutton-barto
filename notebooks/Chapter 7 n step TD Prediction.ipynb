{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce3c42d-b46f-4d60-96b8-4b562787d67c",
   "metadata": {},
   "source": [
    "### 7.1\n",
    "In Chapter 6 we noted that the Monte Carlo error can be written as the\n",
    "sum of $TD$ errors (6.6) if the value estimates don’t change from step to step. Show that\n",
    "the *n*-step error used in (7.2) can also be written as a sum $TD$ errors (again if the value\n",
    "estimates don’t change) generalizing the earlier result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb0a28f-6f7e-436e-9a25-96cb61eb1b52",
   "metadata": {},
   "source": [
    "Value estimates don't change, so: $V_t(S_t) = V_k(S_t), for\\, all\\, k,n \\in \\mathbb{N}_+$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8870c05-c59c-461f-9392-9408eefb2a5d",
   "metadata": {},
   "source": [
    "$G_{t:t+n} - V_{t+n-1}(S_t) = R_{t+1} + \\gamma R_{t+2} + ... +$\n",
    "$\\gamma^{n-1}R_{t+n} + \\gamma^n V_{t+n-1}(S_{t+n}) - V_{t+n-1}(S_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51030a2b-12ea-4511-a04b-32daff4b3202",
   "metadata": {},
   "source": [
    "&emsp;$= R_{t+1} + \\gamma R_{t+2} + ... +$\n",
    "$\\gamma^{n-1}R_{t+n} + \\gamma^n V_{t+n-1}(S_{t+n}) - V_{t+n-1}(S_t) + V_t(S_{t+1}) - $\n",
    "$V_t(S_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ae17d-afcf-4db5-bcf7-c593330132c7",
   "metadata": {},
   "source": [
    "&emsp;$=\\delta_t+ \\gamma R_{t=2} + ...+\\gamma^{n-1}R_{t+n} +$\n",
    "$\\gamma^nV_{t+n-1}(S_{t+n}) -V_t(S_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0477b540-c9bd-431d-8884-14ead023e04c",
   "metadata": {},
   "source": [
    "&emsp;$=\\delta_t+ \\gamma R_{t=2} + ...+\\gamma^{n-1}R_{t+n} +$\n",
    "$\\gamma^nV_{t+n-1}(S_{t+n}) -V_t(S_{t+1}) + \\gamma V_t(S_{t+2}) - \\gamma V_t(S_{t+2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d359cc3-7a07-41d1-ba81-924f4d1507dc",
   "metadata": {},
   "source": [
    "&emsp;$=\\delta_t + \\gamma\\delta_{t+1} ...+\\gamma^{n-1}R_{t+n} +$\n",
    "$\\gamma^nV_{t+n-1}(S_{t+n}) - \\gamma V_t(S_{t+2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ba638f-3d9d-43e8-ac0d-a98f83b93c6b",
   "metadata": {},
   "source": [
    "&emsp;$=\\delta_t + \\gamma\\delta_{t+1} ...+ \\gamma^{n-1}\\delta_{t+n} +$\n",
    "$\\gamma^nV_{t+n-1}(S_{t+n}) - \\gamma^n V_t(S_{t+n})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805f120-1b1a-4f7f-88dd-92a158518923",
   "metadata": {},
   "source": [
    "&emsp;$=\\sum_{k=t}^{t+n}\\gamma^{t-k}\\delta_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef8a53-0a9b-404a-bbd3-7297ed9a539c",
   "metadata": {},
   "source": [
    "### 7.2\n",
    "(programming) With an *n*-step method, the value estimates do change from\n",
    "step to step, so an algorithm that used the sum of $TD$ errors (see previous exercise) in\n",
    "place of the error in (7.2) would actually be a slightly different algorithm. Would it be a\n",
    "better algorithm or a worse one? Devise and program a small experiment to answer this\n",
    "question empirically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3d9a98-f27d-464b-ba6e-9872eb7c075c",
   "metadata": {},
   "source": [
    "### 7.3\n",
    "Why do you think a larger random walk task ($19$ states instead of $5$) was\n",
    "used in the examples of this chapter? Would a smaller walk have shifted the advantage\n",
    "to a different value of *n*? How about the change in left-side outcome from $0$ to $-1$ made\n",
    "in the larger walk? Do you think that made any difference in the best value of *n*?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7272b27-3b1b-46bf-835c-1e0580a2429f",
   "metadata": {},
   "source": [
    "Smaller walk would require less predicting of the future so we will not benefit from\n",
    "larger step, and i think it would shift the advantage to n=2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ccec4-431f-4bda-ae91-575fd812476a",
   "metadata": {},
   "source": [
    "I think that changing left outcome would mean that smaller values of n are more efficient\n",
    "than before since we will have change of value if we go left since there reward will\n",
    "be other than 0, so a bigger change will occur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902fbd0c-bb0f-4be4-be64-b3011de19b81",
   "metadata": {},
   "source": [
    "### 7.4\n",
    "Prove that the n-step return of Sarsa (7.4) can be written exactly in terms\n",
    "of a novel TD error, as\n",
    "$$G_{t:t+n} = Q_{t-1}(S_t,A_t) + \\sum_{k=t}^{min(t+n,T)-1} \\gamma^{k-t}[R_{k+1}+\\gamma Q_k(S_{k+1},A_{k+1})-Q_{k-1}(S_k,A_k)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdf5bb7-c846-4f2a-ba86-bdf0af3850e1",
   "metadata": {},
   "source": [
    "$G_{t:t+n} - Q_{t+n-1}(S_t, A_t) =$  \n",
    "&emsp;$Q_{t-1}(S_t,A_t) + \\sum_{k=t}^{min(t+n,T)-1} \\gamma^{k-t}[R_{k+1}+\\gamma Q_k(S_{k+1},A_{k+1})-Q_{k-1}(S_k,A_k)] - Q_{t-1}(S_t, A_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f6da76-1d9e-4ece-bc24-a0010dc69cf5",
   "metadata": {},
   "source": [
    "&emsp;$=Q_{t-1}(S_t,A_t) + R_{t+1} + \\gamma Q_t(S_{t+1}, A_{t+1}) - Q_{t-1}(S_t, A_t) - Q_{t-1}(S_t, A_t) + $\n",
    "$\\sum_{k=t+1}^{min(t+n,T)-1} \\gamma^{k-t}[R_{k+1}+\\gamma Q_k(S_{k+1},A_{k+1})-Q_{k-1}(S_k,A_k)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d8a15c-00e8-41d6-b4c5-08d3d0d8bda1",
   "metadata": {},
   "source": [
    "&emsp;$= \\delta_t + \\sum_{k=t+1}^{min(t+n,T)-1} $\n",
    "$\\gamma^{k-t}[R_{k+1}+\\gamma Q_k(S_{k+1},A_{k+1})-Q_{k-1}(S_k,A_k)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d483d4-5939-429f-b6c6-0a0251339490",
   "metadata": {},
   "source": [
    "&emsp;$= \\delta_t + \\sum_{k=t+1}^{min(t+n,T)-1} $\n",
    "$\\gamma^{k-t}\\delta_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e503bec7-ede5-47b6-b7a5-95bb9cf92085",
   "metadata": {},
   "source": [
    "&emsp;$= \\sum_{k=t}^{min(t+n,T)-1} $\n",
    "$\\gamma^{k-t}\\delta_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb86d7-f0dc-4c0b-8638-d4256b51c2d2",
   "metadata": {},
   "source": [
    "### 7.5 (hard)\n",
    "Write the pseudocode for the off-policy state-value prediction algorithm\n",
    "described above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68761e3a-b530-459a-8e79-dc8dd892c732",
   "metadata": {},
   "source": [
    "### 7.6 (hard)\n",
    "$G_{t:h} = R_{t+1} + \\gamma \\left(\\rho_{t+1}G_{t+1:h} +\\bar{V}_{h-1}(S_{t+1})-\\rho_{t+1}Q_{h-1}(S_{t+1}, A_{t+1})\\right)$  \n",
    "&emsp;$=R_{t+1} + \\gamma \\rho_{t+1}\\left(G_{t+1:h} - Q_{h+1}(S_{t+1}, A_{t+1}) \\right)$\n",
    "$+\\gamma \\bar{V}_{h-1}(S_{t+1})$,${t < h <= T}_{(7.14)}$.  \n",
    "Prove that the control variate in the above equations does not change the\n",
    "expected value of the return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be6a40-e782-4476-afcd-724afb37d876",
   "metadata": {},
   "source": [
    "### 7.7 (hard)\n",
    "Write the pseudocode for the off-policy action-value prediction algorithm\n",
    "described immediately above. Pay particular attention to the termination conditions for\n",
    "the recursion upon hitting the horizon or the end of episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc097b8-a288-4183-96b0-74a9e7f2e792",
   "metadata": {},
   "source": [
    "### 7.8\n",
    "Show that the general (off-policy) version of the *n*-step return (7.13) can\n",
    "still be written exactly and compactly as the sum of state-based $TD$ errors (6.5) if the\n",
    "approximate state value function does not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1f870-a9f4-41a6-85d9-e033d545516d",
   "metadata": {},
   "source": [
    "### 7.9 (hard)\n",
    "Repeat the above exercise for the action version of the off-policy *n*-step\n",
    "return (7.14) and the Expected Sarsa $TD$ error (the quantity in brackets in Equation 6.9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d94396-c234-4fea-9e3e-21f5161eda58",
   "metadata": {},
   "source": [
    "### 7.10\n",
    "(programming) Devise a small off-policy prediction problem and use it to\n",
    "show that the off-policy learning algorithm using (7.13) and (7.2) is more data efficient\n",
    "than the simpler algorithm using (7.1) and (7.9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21f313-989c-422a-b0bb-992163757941",
   "metadata": {},
   "source": [
    "### 7.11\n",
    "Show that if the approximate action values are unchanging, then the\n",
    "tree-backup return (7.16) can be written as a sum of expectation-based TD errors:\n",
    "$$G_{t:t+n}=Q(S_t, A_t) + \\sum_{k=t}^{min(t+n-1,T-1)} \\delta_k \\prod_{i=t+1}^k\\gamma\\pi(A_i|S_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ad364-3768-48ab-ae1f-287dd2bc8220",
   "metadata": {},
   "source": [
    "$G_{t:t+n} = R_{t+1} + \\gamma \\sum_{a \\ne A_{t+1}}\\pi$\n",
    "$(a|S_{t+1})Q_{t+n-1}(S_{t+1},a) + \\gamma\\pi(A_{t+1}|$\n",
    "$S_{t+1})G_{t+1:t+n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e224cb9-d18e-48ab-becf-817e9e5e8b5e",
   "metadata": {},
   "source": [
    "$\\bar{V}_t(s) = \\sum_a \\pi(a|s)Q_t(s,a),$\n",
    "${\\, for\\, all\\, s \\in S}_{(7.8)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43977d7d-5214-4540-95b7-340e7085ea95",
   "metadata": {},
   "source": [
    "$\\delta_t = R_{t+1} + \\gamma \\bar V_t (S_{t+1}) - $\n",
    "$Q(S_t, A_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d2bfa-d424-4903-b3b5-e34887730c37",
   "metadata": {},
   "source": [
    "&emsp;$=Q(S_t,A_t) + R_{t+1} + \\gamma$\n",
    "$\\sum_{a \\ne A_{t+1}}\\pi$\n",
    "$(a|S_{t+1})Q(S_{t+1},a) + \\gamma\\pi(A_{t+1}|$\n",
    "$S_{t+1})[R_{t+2} + \\gamma \\sum_{a' \\ne A_{t+2}}$\n",
    "$\\pi(a'|S_{t+2})Q(S_{t+2}, a') +$\n",
    "$\\pi(A_{t+2}|S_{t+2}]G_{t+2:t+n}$\n",
    "$- Q(S_t, A_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b05574-8cb3-4578-9112-0614caf57af9",
   "metadata": {},
   "source": [
    "&emsp;$=Q(S_t,A_t) + R_{t+1} + \\gamma$\n",
    "$\\sum_{a}\\pi$\n",
    "$(a|S_{t+1})Q(S_{t+1},a) + \\gamma\\pi(A_{t+1}|$\n",
    "$S_{t+1})[R_{t+2} - Q(S_{t+1}, A_{t+1})$\n",
    "$+ \\gamma \\sum_{a' \\ne A_{t+2}}$\n",
    "$\\pi(a'|S_{t+2})Q(S_{t+2}, a') +$\n",
    "$\\pi(A_{t+2}|S_{t+2}]G_{t+2:t+n}$\n",
    "$- Q(S_t, A_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b9039-e02c-4fb4-b66e-972a2eddfcce",
   "metadata": {},
   "source": [
    "&emsp;$=Q(S_t,A_t) + \\delta_t+ \\gamma\\pi(A_{t+1}|$\n",
    "$S_{t+1})(R_{t+2} - Q(S_{t+1}, A_{t+1})$\n",
    "$+ \\gamma \\sum_{a' \\ne A_{t+2}}$\n",
    "$\\pi(a'|S_{t+2})Q(S_{t+2}, a') +$\n",
    "$\\pi(A_{t+2}|S_{t+2})G_{t+2:t+n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a686709-449f-41ba-b29b-75a86dffb87d",
   "metadata": {},
   "source": [
    "&emsp;$=Q(S_t,A_t) + \\delta_t+ \\gamma\\pi(A_{t+1}|$\n",
    "$S_{t+1})(R_{t+2} - Q(S_{t+1}, A_{t+1})$\n",
    "$+ \\gamma \\sum_{a'}$\n",
    "$\\pi(a'|S_{t+2})Q(S_{t+2}, a') +$\n",
    "$\\pi(A_{t+2}|S_{t+2})$\n",
    "$(R_{t+3} - Q(S_{t+2}, A_{t+2})$\n",
    "$+ \\gamma \\sum_{a'' \\ne A_{t+3}}$\n",
    "$\\pi(a'|S_{t+3})Q(S_{t+3}, a') +$\n",
    "$\\pi(A_{t+3}|S_{t+3})G_{t+3:t+n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a387b719-1358-47c0-964e-5879dc6ab465",
   "metadata": {},
   "source": [
    "&emsp;$=Q(S_t,A_t) + \\delta_t+ \\gamma\\pi(A_{t+1}|$\n",
    "$S_{t+1})(\\delta_{t+1}$\n",
    "$\\pi(A_{t+2}|S_{t+2})$\n",
    "$(R_{t+3} - Q(S_{t+2}, A_{t+2})$\n",
    "$+ \\gamma \\sum_{a'' \\ne A_{t+3}}$\n",
    "$\\pi(a'|S_{t+3})Q(S_{t+3}, a') +$\n",
    "$\\pi(A_{t+3}|S_{t+3})G_{t+3:t+n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af4e582-5254-44c5-bd5a-960a5567603f",
   "metadata": {},
   "source": [
    "&emsp;$=Q(S_t,A_t) + \\delta_t+ \\gamma\\pi(A_{t+1}|$\n",
    "$S_{t+1})(\\delta_{t+1}$\n",
    "$\\pi(A_{t+2}|S_{t+2})$\n",
    "$(R_{t+3} - Q(S_{t+2}, A_{t+2})$\n",
    "$+ \\gamma \\sum_{a'' \\ne A_{t+3}}$\n",
    "$\\pi(a'|S_{t+3})Q(S_{t+3}, a') +$\n",
    "$\\pi(A_{t+3}|S_{t+3})G_{t+3:t+n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309898f-baea-4a73-ba41-8a45c0aaee59",
   "metadata": {},
   "source": [
    "$=...=Q(S_t,A_t) + \\delta_t + \\gamma\\pi(A_{t+1}| S_{t+1})$\n",
    "$\\delta_{t+1} + \\gamma\\pi(A_{t+1}| S_{t+1})$\n",
    "$+\\gamma\\pi(A_{t+1})\\gamma\\pi(A_{t+2}|S_{t+2})\\delta_{t+2}$\n",
    "$+...+G_{T:T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c6cc42-5494-4c53-8f3f-315f7a858392",
   "metadata": {},
   "source": [
    "&emsp;$=Q(S_t, A_t) + \\sum_{k=t}^{min(t+n-1, T-1)}$\n",
    "$\\delta_k \\prod_{i=t+1}^k\\gamma\\pi(A_i|S_i)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
