{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa1ad8c5-4529-489d-903e-4183aa19e7c8",
   "metadata": {},
   "source": [
    "### 5.1\n",
    "![Obrazek](5_1.png)  \n",
    "Consider the diagrams on the right in Figure 5.1. Why does the estimated\n",
    "value function jump up for the last two rows in the rear? Why does it drop off for the\n",
    "whole last row on the left? Why are the frontmost values higher in the upper diagrams\n",
    "than in the lower?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57355533-d92a-424b-a143-aafc1e36a3c6",
   "metadata": {},
   "source": [
    "It jumps because we finally stick instead of taking more cards plus it is very good score for blackjack.  \n",
    "It drops off on the left because dealer has advantage with ace since it can get blackjack which beats everything  \n",
    "Frontmost values are higer in upper diagram because player can get an ace and decide how much is it in score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b807f-81ac-471f-8a49-648180949961",
   "metadata": {},
   "source": [
    "### 5.2\n",
    "Suppose every-visit MC was used instead of first-visit MC on the blackjack\n",
    "task. Would you expect the results to be very different? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23024ba-579b-4d4e-ab35-2552039a67e8",
   "metadata": {},
   "source": [
    "It shouldn't make a difference since in one episode we cannot have occurences of one state more than once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a950138b-515a-4272-9660-2c1467ba449d",
   "metadata": {},
   "source": [
    "### 5.3\n",
    "What is the backup diagram for Monte Carlo estimation of $q_\\pi$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8618f2-da46-48e5-a05c-1527eb264a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "dot = graphviz.Digraph(\"\", format=\"jpg\")\n",
    "dot.node('A', '')\n",
    "dot.node('B','')\n",
    "dot.node('C','...')\n",
    "dot.node('D','')\n",
    "dot.node('E','')\n",
    "dot.node('F','...')\n",
    "dot.node('G','')\n",
    "dot.node('H','')\n",
    "dot.node('I','...')\n",
    "dot.node('J','')\n",
    "dot.node('K','')\n",
    "dot.node('L','...')\n",
    "dot.node('M','')\n",
    "dot.edges(['AB', 'AE', 'AH', 'AK', 'BC', 'CD', 'EF', 'FG', 'HI', 'IJ', 'KL', 'LM'])\n",
    "filename = dot.render(filename='5_3')\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95028ff-1eca-4c96-a81d-2f722300457b",
   "metadata": {},
   "source": [
    "![Answer](5_3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa6cb0-6f8e-4995-b50e-49870329daad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5.4\n",
    "The pseudocode for Monte Carlo ES is inefficient because, for each state–\n",
    "action pair, it maintains a list of all returns and repeatedly calculates their mean. It would\n",
    "be more efficient to use techniques similar to those explained in Section 2.4 to maintain\n",
    "just the mean and a count (for each state–action pair) and update them incrementally.\n",
    "Describe how the pseudocode would be altered to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c25a78-211a-49e0-aab0-a19111e65c11",
   "metadata": {},
   "source": [
    "It should change assigment to this:  \n",
    "$Q(S_t, A_t) \\leftarrow G + \\frac{(G - Q(S_t, A_t))}{|Q(S_t, A_t)|-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edcc818-0400-4f13-9e0e-2e2263fa2cb7",
   "metadata": {},
   "source": [
    "### 5.5\n",
    "Consider an MDP with a single nonterminal state and a single action\n",
    "that transitions back to the nonterminal state with probability p and transitions to the\n",
    "terminal state with probability $1-p$. Let the reward be $+1$ on all transitions, and let $\\gamma= 1$. Suppose you observe one episode that lasts $10$ steps, with a return of $10$. What\n",
    "are the first-visit and every-visit estimators of the value of the nonterminal state?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf9529-579c-4c45-acaf-01b837511999",
   "metadata": {},
   "source": [
    "For first-visit it would be: 10  \n",
    "For every-visit it would be: $\\frac{\\sum_{k=1}^{10}}{10} = 5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfeccad-c266-45fe-aa86-cca6c5f76cd2",
   "metadata": {},
   "source": [
    "### 5.6\n",
    "What is the equation analogous to (5.6) for action values $Q(s, a)$ instead of\n",
    "state values $V(s)$, again given returns generated using $b$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba58fa7-51c3-4ffc-8fb3-3e4021881075",
   "metadata": {},
   "source": [
    "$Q(s,a) = \\frac{\\sum_{t \\in T(s)}p_{t+1:T(t)-1}G_t + (R_t | a)}{\\sum_{t \\in T(s)}p_{t:T(t)-1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64cda7d-f3fb-4321-8894-20f1d3b9a176",
   "metadata": {},
   "source": [
    "### 5.7\n",
    "In learning curves such as those shown in Figure 5.3 error generally decreases\n",
    "with training, as indeed happened for the ordinary importance-sampling method. But for\n",
    "the weighted importance-sampling method error first increased and then decreased. Why\n",
    "do you think this happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14f82a0-815b-4cdc-98da-ab6ae335b1d1",
   "metadata": {},
   "source": [
    "It's because beginning timesteps are biased by the behaviour policy, so we firstly take our guesses from the behaviour rather than target policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1953b5-b70b-4674-aea2-fd8e6306cddd",
   "metadata": {},
   "source": [
    "### 5.8\n",
    "The results with Example 5.5 and shown in Figure 5.4 used a first-visit MC\n",
    "method. Suppose that instead an every-visit MC method was used on the same problem.\n",
    "Would the variance of the estimator still be infinite? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8f16c9-028b-4906-bb72-870b788ead1d",
   "metadata": {},
   "source": [
    "Every-visit MC would still have infinite variance since in the beginning nominator and denominator will cancel out so it will behave just like the first-visit MC in the beginning where the variance it the biggest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28edc1a6-d392-4ff5-a1e8-ee97b301280c",
   "metadata": {},
   "source": [
    "### 5.9\n",
    "Modify the algorithm for first-visit MC policy evaluation (Section 5.1) to\n",
    "use the incremental implementation for sample averages described in Section 2.4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4181e9-d139-41b9-a30d-25d92037cc9b",
   "metadata": {},
   "source": [
    "$V(S_t) \\leftarrow R_t + \\frac{1}{n}(R_t - V(S_t))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6e9f9-5269-4973-aff7-2775a2c7fc60",
   "metadata": {},
   "source": [
    "### 5.10\n",
    "Derive the weighted-average update rule (5.8) from (5.7). Follow the\n",
    "pattern of the derivation of the unweighted rule (2.3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1518de-d63a-4146-b184-a95dd6a19644",
   "metadata": {},
   "source": [
    "$V_{n+1}=\\frac{\\sum_{k=1}^{n}W_kG_k}{\\sum_{k=1}^{n}W_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d09391-f2a8-4320-a5d5-c28783b03170",
   "metadata": {},
   "source": [
    "$V_{n+1}=\\frac{W_nG_n+\\sum_{k=1}^{n-1}W_kG_k}{W_n+\\sum_{k=1}^{n-1}W_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f8a35e-5fda-4722-a8ad-793c73d61301",
   "metadata": {},
   "source": [
    "$V_{n+1}=\\frac{\\sum_{k=1}^{n-1}W_kG_k}{W_n+\\sum_{k=1}^{n-1}W_k} + \\frac{W_nG_n}{W_n+\\sum_{k=1}^{n-1}W_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a3ee1-6424-4a25-b6c4-06a1a08aa4ac",
   "metadata": {},
   "source": [
    "$V_{n+1}=(\\sum_{k=1}^{n-1}W_k)\\frac{1}{\\sum_{k=1}^{n-1}W_k}\\frac{\\sum_{k=1}^{n-1}W_kG_k}{W_n+\\sum_{k=1}^{n-1}W_k} + \\frac{W_nG_n}{W_n+\\sum_{k=1}^{n-1}W_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9fc4a4-796e-4cbf-981f-c26eebdde51e",
   "metadata": {},
   "source": [
    "$V_{n+1}=V_n\\frac{\\sum_{k=1}^{n-1}W_k}{W_n+\\sum_{k=1}^{n-1}W_k} + \\frac{W_nG_n}{W_n+\\sum_{k=1}^{n-1}W_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97bbcb5-1480-41f1-b911-9495c0e9b99d",
   "metadata": {},
   "source": [
    "$V_{n+1}=\\frac{V_n\\sum_{k=1}^{n-1}W_k+W_nG_n}{W_n+\\sum_{k=1}^{n-1}W_k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e4007-13fa-4433-bbe3-01fb42fa8e55",
   "metadata": {},
   "source": [
    "### 5.11\n",
    "In the boxed algorithm for off-policy MC control, you may have been\n",
    "expecting the $W$ update to have involved the importance-sampling ratio\n",
    "$\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)}$, but instead it involves $\\frac{1}{b(A_t|S_t)}$. Why is this\n",
    "nevertheless correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9cf4dc-befb-486e-8499-073adb9c1cb2",
   "metadata": {},
   "source": [
    "It's because the target policy is deterministic, so it will always select this action in a particular state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec01538a-49d7-4b9f-8c85-23156831a8ff",
   "metadata": {},
   "source": [
    "### 5.12\n",
    "Racetrack (programming) Consider driving a race car around a turn\n",
    "like those shown in Figure 5.5. You want to go as fast as possible, but not so fast as\n",
    "to run off the track. In our simplified racetrack, the car is at one of a discrete set of\n",
    "grid positions, the cells in the diagram. The velocity is also discrete, a number of grid\n",
    "cells moved horizontally and vertically per time step. The actions are increments to the\n",
    "velocity components. Each may be changed by $+1, 1,$ or $0$ in each step, for a total of\n",
    "nine $(3 \\times 3)$ actions. Both velocity components are restricted to be nonnegative and less\n",
    "than $5$, and they cannot both be zero except at the starting line. Each episode begins\n",
    "in one of the randomly selected start states with both velocity components zero and\n",
    "ends when the car crosses the finish line. The rewards are $1$ for each step until the car\n",
    "crosses the finish line. If the car hits the track boundary, it is moved back to a random\n",
    "position on the starting line, both velocity components are reduced to zero, and the episode continues. Before updating the car’s location at each time step, check to see if\n",
    "the projected path of the car intersects the track boundary. If it intersects the finish line,\n",
    "the episode ends; if it intersects anywhere else, the car is considered to have hit the track\n",
    "boundary and is sent back to the starting line. To make the task more challenging, with\n",
    "probability $0.1$ at each time step the velocity increments are both zero, independently of\n",
    "the intended increments. Apply a Monte Carlo control method to this task to compute\n",
    "the optimal policy from each starting state. Exhibit several trajectories following the\n",
    "optimal policy (but turn the noise off for these trajectories)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3742ba5d-6781-4702-af05-e4958ffc80f9",
   "metadata": {},
   "source": [
    "### 5.13 (hard)\n",
    "Show the steps to derive (5.14) from (5.12)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c322a1-67b6-4b45-a95a-bb78b79733c4",
   "metadata": {},
   "source": [
    "### 5.14(hard)\n",
    "Modify the algorithm for off-policy Monte Carlo control (page 111) to use\n",
    "the idea of the truncated weighted-average estimator (5.10). Note that you will first need\n",
    "to convert this equation to action values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff0d39b-dfee-470e-935d-e19719d3ada4",
   "metadata": {},
   "source": [
    "### 5.15\n",
    "Make new equations analogous to the importance-sampling Monte Carlo\n",
    "estimates $(5.5)$ and $(5.6)$, but for action value estimates $Q(s, a)$. You will need new\n",
    "notation $T(s, a)$ for the time steps on which the state–action pair $s, a$ is visited on the\n",
    "episode. Do these estimates involve more or less importance-sampling correction?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
