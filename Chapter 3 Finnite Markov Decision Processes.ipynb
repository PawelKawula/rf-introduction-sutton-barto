{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7138e5b-0575-4431-b69d-4026c6bcaea7",
   "metadata": {},
   "source": [
    "### 3.1\n",
    "Devise three example tasks of your own that fit into the MDP framework,\n",
    "identifying for each its states, actions, and rewards. Make the three examples as *different*\n",
    "from each other as possible. The framework is abstract and flexible and can be applied in\n",
    "many *different* ways. Stretch its limits in some way in at least one of your examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb082c-c1af-450e-a629-91a478a376a4",
   "metadata": {},
   "source": [
    "### 3.2\n",
    "Is the MDP framework adequate to usefully represent *all* goal-directed\n",
    "learning tasks? Can you think of any clear exceptions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275e360-cd72-4f02-9522-90f663b31033",
   "metadata": {},
   "source": [
    "### 3.3\n",
    "Consider the problem of driving. You could define the actions in terms of\n",
    "the accelerator, steering wheel, and brake, that is, where your body meets the machine.\n",
    "Or you could define them farther out—say, where the rubber meets the road, considering\n",
    "your actions to be tire torques. Or you could define them farther in—say, where your\n",
    "brain meets your body, the actions being muscle twitches to control your limbs. Or you\n",
    "could go to a really high level and say that your actions are your choices of where to drive.\n",
    "What is the right level, the right place to draw the line between agent and environment?\n",
    "On what basis is one location of the line to be preferred over another? Is there any\n",
    "fundamental reason for preferring one location over another, or is it a free choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36c36f-0764-4ea3-b32e-759af7762195",
   "metadata": {},
   "source": [
    "### 3.4\n",
    "Give a table analogous to that in Example 3.3, but for $p(s',r |s, a)$. It\n",
    "should have columns for $s, a, s' , r,$ and $p(s' , r |s, a)$, and a row for every 4-tuple for which\n",
    "$p(s 0 , r |s, a) > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307e7e79-132d-41e9-b0ec-4cd59dc3d00f",
   "metadata": {},
   "source": [
    "### 3.5\n",
    "Exercise 3.5 The equations in Section 3.1 are for the continuing case and need to be\n",
    "modified (very slightly) to apply to episodic tasks. Show that you know the modifications\n",
    "needed by giving the modified version of (3.3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da263000-54c9-4f2f-86a3-c24a3f3c17a8",
   "metadata": {},
   "source": [
    "### 3.6\n",
    "Suppose you treated pole-balancing as an episodic task but also used\n",
    "discounting, with all rewards zero except for 1 upon failure. What then would the\n",
    "return be at each time? How does this return di↵er from that in the discounted, continuing\n",
    "formulation of this task?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887a626-59e7-4a45-a7cf-380aed6a8b95",
   "metadata": {},
   "source": [
    "### 3.7\n",
    "Exercise 3.7 Imagine that you are designing a robot to run a maze. You decide to give it a\n",
    "reward of +1 for escaping from the maze and a reward of zero at all other times. The task\n",
    "seems to break down naturally into episodes—the successive runs through the maze—so\n",
    "you decide to treat it as an episodic task, where the goal is to maximize expected total\n",
    "reward (3.7). After running the learning agent for a while, you find that it is showing\n",
    "no improvement in escaping from the maze. What is going wrong? Have you effectively\n",
    "communicated to the agent what you want it to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2432c9d-e616-4d68-99a4-fff0e69364c2",
   "metadata": {},
   "source": [
    "### 3.8\n",
    "Suppose = 0.5 and the following sequence of rewards is received $R_1 = 1,\n",
    "R_2 = 2, R_3 = 6, R_4 = 3$, and $R_5 = 2$, with $T = 5$. What are $G_0 , G_1 , . . ., G_5$ ? Hint:\n",
    "Work backwards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a34ff-c97f-4558-affa-65bf21a37ba3",
   "metadata": {},
   "source": [
    "### 3.9\n",
    "Exercise 3.9 Suppose $\\gamma= 0.9$ and the reward sequence is $R_1 = 2$ followed by an infinite\n",
    "sequence of 7s. What are $G_1$ and $G_0$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7775e3be-eb05-48ba-b52a-8ddad01475b8",
   "metadata": {},
   "source": [
    "### 3.10\n",
    "Exercise 3.10 Prove the second equality in (3.10)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce133f-9da0-4f81-b155-74625dafe622",
   "metadata": {},
   "source": [
    "### 3.11\n",
    "If the current state is $S_t$ , and actions are selected according to a stochastic\n",
    "policy $\\pi$, then what is the expectation of $R_{t+1}$ in terms of $\\pi$ and the four-argument\n",
    "function $p$ (3.2)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d548cb7-6b88-4d68-9808-ed0c1dbf2633",
   "metadata": {},
   "source": [
    "### 3.12\n",
    "Exercise 3.12 Give an equation for $v_\\pi$ in terms of $q_\\pi$ and $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f379b3cd-dc89-4dc8-9ef2-a83ee59d0c5c",
   "metadata": {},
   "source": [
    "### 3.13\n",
    "Exercise 3.13 Give an equation for $q_\\pi$ in terms of $v_\\pi$ and the four-argument $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416e1ec-c915-4d6c-9cc8-7addd37a06d5",
   "metadata": {},
   "source": [
    "### 3.14\n",
    "Exercise 3.14 The Bellman equation (3.14) must hold for each state for the value function\n",
    "$v_\\pi$ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds\n",
    "for the center state, valued at $+0.7$, with respect to its four neighboring states, valued at\n",
    "$+2.3$, $+0.4$, $0.4$, and $+0.7$. (These numbers are accurate only to one decimal place.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc31f4-c7e9-4c2a-806a-d1da503775e0",
   "metadata": {},
   "source": [
    "### 3.15\n",
    "In the gridworld example, rewards are positive for goals, negative for\n",
    "running into the edge of the world, and zero the rest of the time. Are the signs of these\n",
    "rewards important, or only the intervals between them? Prove, using (3.8), that adding a\n",
    "constant c to all the rewards adds a constant, v c , to the values of all states, and thus\n",
    "does not a↵ect the relative values of any states under any policies. What is v c in terms\n",
    "of c and ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
