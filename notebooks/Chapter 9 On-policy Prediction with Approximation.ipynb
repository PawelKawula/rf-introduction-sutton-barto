{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a186c91f-7eb7-4822-8f13-dab321d62834",
   "metadata": {},
   "source": [
    "### 1\n",
    "Show that tabular methods such as presented in Part I of this book are a\n",
    "special case of linear function approximation. What would the feature vectors be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12035ec-6fa0-4db6-b735-e8e46723c976",
   "metadata": {},
   "source": [
    "That's because instead of each of the element of weight vector corresponding to couple of vectors we\n",
    "have each element corresponding to one distinct state.\n",
    "Feature vector would be just an index to corresponding q or v value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9affd3f-37fb-4df1-8688-9c773fa486b2",
   "metadata": {},
   "source": [
    "### 2\n",
    "Why does (9.17) define $(n + 1)^k$ distinct features for dimension $k$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f0c0e-1bbc-4763-8388-19aa12a57fc6",
   "metadata": {},
   "source": [
    "It's because in every ocasion we have $(x_1^0\\cdot x_2^0\\cdot...\\cdot x_n^0,$\n",
    "$x_1^1\\cdot x_2^0\\cdot...x_n^0,..., x_1^k\\cdot x_2^k \\cdot ... \\cdot x_n^k$   \n",
    "So that describes variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7ad81-c61c-48fd-ae6a-0943a03a7f50",
   "metadata": {},
   "source": [
    "### 3\n",
    "What $n$ and $c_{i,j}$ produce the feature vectors $x(s) =$\n",
    "$(1, s_1, s_2, s_1s_2, s_1^2, s_2^2, s_1s_2^2, s_1^2s_2, s_1^2s_2^2)^T $?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9250fec-8c65-4e1e-a8e1-c234bfffda5b",
   "metadata": {},
   "source": [
    "$n=2, c_{i,j} = \\lbrace\\color{green}{0,} 1, 2 \\rbrace$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7017e822-e5b4-4071-8a15-2a0a4e6a1759",
   "metadata": {},
   "source": [
    "### 4\n",
    "Suppose we believe that one of two state dimensions is more likely to have\n",
    "an effect on the value function than is the other, that generalization should be primarily\n",
    "across this dimension rather than along it. What kind of tilings could be used to take\n",
    "advantage of this prior knowledge?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca82698-73f6-49b7-b61a-3698da1e967a",
   "metadata": {},
   "source": [
    "Axes for tilings should be perpendicular to this axis, and maybe some diagonal where we want values to be more precise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36934929-9440-4d75-a8c6-777075f7fbf6",
   "metadata": {},
   "source": [
    "### 5\n",
    "Suppose you are using tile coding to transform a seven-dimensional continuous\n",
    "state space into binary feature vectors to estimate a state value function $vÌ‚(s,w) \\approx v_\\pi (s)$.\n",
    "You believe that the dimensions do not interact strongly, so you decide to use eight tilings\n",
    "of each dimension separately (stripe tilings), for $7 x 8 = 56$ tilings. In addition, in case\n",
    "there are some pairwise interactions between the dimensions, you also take all $\\begin{pmatrix}7\\\\2\\end{pmatrix} = 21$\n",
    "pairs of dimensions and tile each pair conjunctively with rectangular tiles. You make\n",
    "two tilings for each pair of dimensions, making a grand total of $21 x 2 + 56 = 98$ tilings.\n",
    "Given these feature vectors, you suspect that you still have to average out some noise,\n",
    "so you decide that you want learning to be gradual, taking about 10 presentations with\n",
    "the same feature vector before learning nears its asymptote. What step-size parameter\n",
    "should you use? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb4b19c-1ebc-4cfb-9659-96c21e49f982",
   "metadata": {},
   "source": [
    "Since we are using stripe tilings we would haave 7 stripe tailings (one for each dimensions) for every\n",
    "sample we get, additionaly we add 21 to our sum since that is the number of intersections between these\n",
    "dimensions, and since our result of $\\mathbb{E}[x^Tx]$ is equal to $\\sum_{k=1}^{7+21}=28$ we would use step-size of\n",
    "$2.8$ to approach asymptote in 10 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7bb473-05cb-40f7-bddb-2822e18a5ade",
   "metadata": {},
   "source": [
    "IDK Answers were very different but without logic so..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad89658-7389-46f6-a953-7e7af967b1a1",
   "metadata": {},
   "source": [
    "### 6\n",
    "If $\\tau = 1$ and $x(S_t )^T x(S_t ) = \\mathbb{E} [x^T x]$, prove that (9.19) together with (9.7)\n",
    "and linear function approximation results in the error being reduced to zero in one update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f84b0-4608-47a7-96d8-05105f614d1f",
   "metadata": {},
   "source": [
    "Since it  is a linear function updae reduces to:\n",
    "$w_{t+1} = w_t + \\frac1\\tau \\left[U_t - \\bar v(S_t, w_t)\\nabla \\bar v(S_t, W_t) \\right]$  \n",
    "&emsp;$=w_t + \\left[ U_t - \\bar v(S_t, w_t)\\right]$\n",
    "Adding a difference between desired value and old value basically means that we get desired value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0a809-3c41-4f58-9f0c-7e36063032ed",
   "metadata": {},
   "source": [
    "### 7\n",
    "One of the simplest artificial neural networks consists of a single semi-linear\n",
    "unit with a logistic nonlinearity. The need to handle approximate value functions of this\n",
    "form is common in games that end with either a win or a loss, in which case the value of\n",
    "a state can be interpreted as the probability of winning. Derive the learning algorithm\n",
    "for this case, from (9.7), such that no gradient notation appears."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e78970-d2b1-4831-84e1-577a98b36236",
   "metadata": {},
   "source": [
    "${w_{t+1} = w_t + \\alpha \\left[ U_t - \\bar v(S_t, w_t)\\right] \\nabla \\bar v(S_t, w_t)}_{(9.7)}$\n",
    "&emsp;$=$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7415a41-9df4-419f-a39f-900e9a389014",
   "metadata": {},
   "source": [
    "### 8 (hard)\n",
    "Arguably, the squared error used to derive (9.7) is inappropriate for the\n",
    "case treated in the preceding exercise, and the right error measure is the cross-entropy\n",
    "loss (which you can find on Wikipedia). Repeat the derivation in Section 9.3, using the\n",
    "cross-entropy loss instead of the squared error in (9.4), all the way to an explicit form\n",
    "with no gradient or logarithm notation in it. Is your final form more complex, or simpler,\n",
    "than that you obtained in the preceding exercise?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
