{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5ebae33-7481-4ba8-b2a2-a9668f262470",
   "metadata": {},
   "source": [
    "### 1.1\n",
    "*Self-Play* Suppose, instead of playing against a random opponent, the\n",
    "reinforcement learning algorithm described above played against itself, with both sides\n",
    "learning. What do you think would happen in this case? Would it learn a different policy\n",
    "for selecting moves?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8c4a93-ae41-4d3c-acd9-6b2726bc6802",
   "metadata": {},
   "source": [
    "I belive it would continually change it's policy to fight\n",
    "aganist the opponent, since we will continually update our value function \n",
    "to match current situation, so when our oponent adapts our program \n",
    "will try to adapt aswell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead6564-dc9c-4e25-8005-97f8693311e0",
   "metadata": {},
   "source": [
    "### 1.2\n",
    "*Symmetries* Many tic-tac-toe positions appear different but are really\n",
    "the same because of symmetries. How might we amend the learning process described\n",
    "above to take advantage of this? In what ways would this change improve the learning\n",
    "process? Now think again. Suppose the opponent did not take advantage of symmetries.\n",
    "In that case, should we? Is it true, then, that symmetrically equivalent positions should\n",
    "necessarily have the same value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf93481-403e-43fb-854c-0523dfdd69a0",
   "metadata": {},
   "source": [
    "Ultimately both should have the same value. We can try to compare states by fe. flipping it and seeing if we have this state already\n",
    "and then act upon our policy as if we had the same state. I don't see why we shouldn't use this to our advantage, unless our opponent \n",
    "doesn't use this and in one action it is behaving poorly even if it should have advantage in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1644a8-9150-4b16-8de6-d6e9f57f2173",
   "metadata": {},
   "source": [
    "### 1.3\n",
    "*Greedy Play* Suppose the reinforcement learning player was greedy, that is,\n",
    "it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a nongreedy player? What problems might occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99febe-5c58-4407-9876-c768ea29d6db",
   "metadata": {},
   "source": [
    "It would most likely perform better in the beginning, but as the time progresses \n",
    "it would be hindered by lack of knowledge since it would not learn true value \n",
    "function or it's approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dba80f-c1df-40aa-9012-3aeba171b44f",
   "metadata": {},
   "source": [
    "### 1.4\n",
    "*Learning from Exploration* Suppose learning updates occurred after all\n",
    "moves, including exploratory moves. If the step-size parameter is appropriately reduced\n",
    "over time (but not the tendency to explore), then the state values would converge to\n",
    "a different set of probabilities. What (conceptually) are the two sets of probabilities\n",
    "computed when we do, and when we do not, learn from exploratory moves? Assuming\n",
    "that we do continue to make exploratory moves, which set of probabilities might be better\n",
    "to learn? Which would result in more wins?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b8dc53-f8ce-4e3f-a1b5-3ec67819a666",
   "metadata": {},
   "source": [
    "IDK\n",
    "We should learn from exploratory moves by using greedy ones in each state in this episode so that we can evaluate \n",
    "only this single action, because then we can more accurately asses if it is better if we don't change other vars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c936ac2-b8ee-4252-a990-9db776e668c4",
   "metadata": {},
   "source": [
    "### 1.5\n",
    "*Other Improvements* Can you think of other ways to improve the reinforcement\n",
    "learning player? Can you think of any better way to solve the tic-tac-toe problem\n",
    "as posed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302475e7-9505-41d5-8497-8a14226c0d8a",
   "metadata": {},
   "source": [
    "We can maybe hard-code best beginning: mark center or corner in the first move, since it is always optimal in the beginning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
