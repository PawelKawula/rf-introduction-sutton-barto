{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e228fba-d1a4-43ff-944e-a03976cc2edb",
   "metadata": {
    "id": "0e228fba-d1a4-43ff-944e-a03976cc2edb"
   },
   "source": [
    "#### The probabilites regarding actions, rewards and states:\n",
    "Function that gives us probability of getting exact reward in a state, that is proceeded by particular choosen action and state:  \n",
    "$$p(s', r| s, a) = Pr \\lbrace S_{t}=s', R_t = r |  S_{t-1}=s, A_{t-1}=a\\rbrace $$\n",
    "Function that gives us probability of being a state given previous action and state:\n",
    "$$p(s' | s, a) = \\sum_{r \\in R} p(s', r | s, a) $$\n",
    "Function that gives us expected reward for given action in a state:\n",
    "$$r(s,a) = \\sum_{r \\in R} r \\sum_{s' \\in S} p(s', r| s, a)$$\n",
    "Function that gives us expected reward in a state given previous action in a state:\n",
    "$$ r(s,a,s') = \\sum_{r \\in R} r \\frac{p(s', r|a,s)}{p(s'|s,a)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596cd8e-55aa-4b82-a4dc-996078806c08",
   "metadata": {
    "id": "4596cd8e-55aa-4b82-a4dc-996078806c08"
   },
   "source": [
    "### 3.1\n",
    "Devise three example tasks of your own that fit into the MDP framework,\n",
    "identifying for each its states, actions, and rewards. Make the three examples as *different*\n",
    "from each other as possible. The framework is abstract and flexible and can be applied in\n",
    "many *different* ways. Stretch its limits in some way in at least one of your examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff52396-ead8-43cf-bf4a-5f3e019d2f54",
   "metadata": {},
   "source": [
    "1) Trading bot: the state could include previous prices of different stocks or \n",
    "options, as well as different indicators and commision. the actions would be selling or\n",
    "buying different stocks. The reward would be overall value of our portfolio.\n",
    "2) Car driving: the state could be broad spectrum of metrics including current\n",
    "speed, rain detector as well as 360 deegress set of cameras, actions could\n",
    "be putting on a break or accelerating, changing gears etc. The reward could\n",
    "be staying as close to the road lines as possible while still maintaining\n",
    "acceptable speed.  \n",
    "3) Air conditioning, the state could be current temperature, whether the windows are opened, temperature outside. Actions could be enabling different turbines. The reward would be for staying as close to the desired temperature as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb082c-c1af-450e-a629-91a478a376a4",
   "metadata": {
    "id": "d4cb082c-c1af-450e-a629-91a478a376a4"
   },
   "source": [
    "### 3.2\n",
    "Is the MDP framework adequate to usefully represent *all* goal-directed\n",
    "learning tasks? Can you think of any clear exceptions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3fbb5b-06da-43e3-a50a-6deb383b9d22",
   "metadata": {},
   "source": [
    "We cannot give a reward for creative tasks like creating music"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8275e360-cd72-4f02-9522-90f663b31033",
   "metadata": {
    "id": "8275e360-cd72-4f02-9522-90f663b31033"
   },
   "source": [
    "### 3.3\n",
    "Consider the problem of driving. You could define the actions in terms of\n",
    "the accelerator, steering wheel, and brake, that is, where your body meets the machine.\n",
    "Or you could define them farther out‚Äîsay, where the rubber meets the road, considering\n",
    "your actions to be tire torques. Or you could define them farther in‚Äîsay, where your\n",
    "brain meets your body, the actions being muscle twitches to control your limbs. Or you\n",
    "could go to a really high level and say that your actions are your choices of where to drive.\n",
    "What is the right level, the right place to draw the line between agent and environment?\n",
    "On what basis is one location of the line to be preferred over another? Is there any\n",
    "fundamental reason for preferring one location over another, or is it a free choice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74502f-55c5-495a-97dc-bebc9699eda6",
   "metadata": {},
   "source": [
    "We will have to decide first what goal do we want to achieve. We should not bother with such low-level tasks as muscle twitches, unless we want to achieve the agent that behaves as close to the desired motion as possible, although i think we should consider this as given, that our thougts correspond directly to our muscle twitches. If we want to achieve more abstract goal, fe. following road lines as best as possible with reasonable speed, i think we should consider our car as the agent, putting on a break or accelerating as actions as well as steering, and the rest as the state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36c36f-0764-4ea3-b32e-759af7762195",
   "metadata": {
    "id": "1f36c36f-0764-4ea3-b32e-759af7762195"
   },
   "source": [
    "### 3.4\n",
    "Give a table analogous to that in Example 3.3, but for $p(s',r |s, a)$. It\n",
    "should have columns for $s, a, s' , r,$ and $p(s' , r |s, a)$, and a row for every 4-tuple for which\n",
    "$p(s_0 , r |s, a) > 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc7e25-a4ea-49e6-8ab4-b3343d309663",
   "metadata": {},
   "source": [
    "| $s$      | $a$ | $s'$ | $r$ | $p(s', r | s, a)$ | \n",
    "| -----------: | -----------: | -----------: | -----------: | -----------: |\n",
    "| $high$ | $search$   | $high$ | $r_{search}$ | $\\alpha$ |\n",
    "| $high$ | $search$   | $low$  | $r_{search}$ | $1-\\alpha$ |\n",
    "| $low$  | $search$   | $high$ | $-3$ | $(1-\\beta)$ |\n",
    "| $low$  | $search$   | $low$  | $r_{search}$ | $\\beta$ |\n",
    "| $high$ | $wait$     | $high$ | $r_{wait}$ | $1$ |\n",
    "| $high$ | $wait$     | $low$  | $0$ | $0$ |\n",
    "| $low$  | $wait$     | $high$ | $0$ | $0$ |\n",
    "| $low$  | $wait$     | $low$  | $1$ | $1$ |\n",
    "| $low$  | $recharge$ | $high$ | $0$ | $1$ |\n",
    "| $low$  | $recharge$ | $low$  | $0$ | $0$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307e7e79-132d-41e9-b0ec-4cd59dc3d00f",
   "metadata": {
    "id": "307e7e79-132d-41e9-b0ec-4cd59dc3d00f"
   },
   "source": [
    "### 3.5\n",
    "The equations in Section 3.1 are for the continuing case and need to be\n",
    "modified (very slightly) to apply to episodic tasks. Show that you know the modifications\n",
    "needed by giving the modified version of (3.3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f1c6b-7067-4234-8316-ee8714f53347",
   "metadata": {},
   "source": [
    "(3.3): $\\sum_{s'\\in S}\\sum_{r \\in R}p(s',r|s,a)=1,$ for all $s \\in S, a \\in A(s)$  \n",
    "<span style=\"color:red\">It was about adding termial states: </span>$$\\sum_{s'\\in S^+}\\sum_{r\\in R}p(s',r|s,a)=1,~for~all~s \\in S, a \\in A(s), S= \\lbrace Non-terminal  States \\rbrace, S^+=\\lbrace All States \\rbrace$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da263000-54c9-4f2f-86a3-c24a3f3c17a8",
   "metadata": {
    "id": "da263000-54c9-4f2f-86a3-c24a3f3c17a8"
   },
   "source": [
    "### 3.6\n",
    "Suppose you treated pole-balancing as an episodic task but also used\n",
    "discounting, with all rewards zero except for $-1$ upon failure. What then would the\n",
    "return be at each time? How does this return differ from that in the discounted, continuing\n",
    "formulation of this task?\n",
    "\n",
    "<span style=\"color:red\">The result then would be $-\\gamma^{T-1}$,\n",
    "    for every episode that ended at timestep T,\n",
    "    then we would have a negative number approaching 0, which i think would be less numerically stable for a computer.</span>  \n",
    "<span style=\"color:green\">Remember that $G_t = R_{t+1} + R_{t+2} + ... + R_T$  \n",
    "and with episodic task: $G_t = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{T-t-1}R_T$  \n",
    "and so for this specific case: $G_t = 0 + 0 + ... + \\gamma^{T-t-1}\\cdot-1$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887a626-59e7-4a45-a7cf-380aed6a8b95",
   "metadata": {
    "id": "a887a626-59e7-4a45-a7cf-380aed6a8b95"
   },
   "source": [
    "### 3.7\n",
    "Imagine that you are designing a robot to run a maze. You decide to give it a\n",
    "reward of +1 for escaping from the maze and a reward of zero at all other times. The task\n",
    "seems to break down naturally into episodes‚Äîthe successive runs through the maze‚Äîso\n",
    "you decide to treat it as an episodic task, where the goal is to maximize expected total\n",
    "reward (3.7). After running the learning agent for a while, you find that it is showing\n",
    "no improvement in escaping from the maze. What is going wrong? Have you effectively\n",
    "communicated to the agent what you want it to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd5f06-dc04-439f-8322-f3a0996be567",
   "metadata": {},
   "source": [
    "It may think that running in back and forth between different locations would be benefitial for escaping from a maze, so it will not try to explore different ways to escape since it does not bother with trying the fastest way out just the ones that will eventually succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2432c9d-e616-4d68-99a4-fff0e69364c2",
   "metadata": {
    "id": "e2432c9d-e616-4d68-99a4-fff0e69364c2"
   },
   "source": [
    "### 3.8\n",
    "Suppose $\\gamma= 0.5$ and the following sequence of rewards is received $R_1 = 1,\n",
    "R_2 = 2, R_3 = 6, R_4 = 3$, and $R_5 = 2$, with $T = 5$. What are $G_0 , G_1 , . . ., G_5$ ? Hint:\n",
    "Work backwards.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ddf41-2c9e-4e97-a4ea-9f91d5b6133a",
   "metadata": {},
   "source": [
    "$G_5 = 2 \\cdot \\gamma^5 = \\frac{1}{16} $  \n",
    "$G_4 = 3 + \\gamma \\cdot \\frac{1}{32} = 3 \\frac{1}{32}$  \n",
    "$G_3 = 6 + \\gamma \\cdot 3 \\frac{1}{32} = 9 \\frac{1}{2^6}$  \n",
    "$G_2 = 2 + \\gamma \\cdot 9 \\frac{1}{2^6} = 11 \\frac{1}{2^7}$  \n",
    "$G_1 = 1 + \\gamma \\cdot 11 \\frac{1}{2^7} = 12 \\frac{1}{2^8}$  \n",
    "<span style=\"color:red\">Correct:</span>  \n",
    "$G_5 = R_6 = 0 (terminal)$  \n",
    "$G_4 = R_5 + G_5 = 2$  \n",
    "$G_3 = R_4 + \\gamma G_4 = 3 + 1 = 4$  \n",
    "$G_2 = R_3 + \\gamma G_3 = 6 + 2 = 8$  \n",
    "$G_1 = R_2 + \\gamma G_2 = 2 + 4 = 6$  \n",
    "$G_0 = R_1 + \\gamma G_1 = -1 + 3 = 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4a34ff-c97f-4558-affa-65bf21a37ba3",
   "metadata": {
    "id": "dc4a34ff-c97f-4558-affa-65bf21a37ba3"
   },
   "source": [
    "### 3.9\n",
    "Suppose $\\gamma= 0.9$ and the reward sequence is $R_1 = 2$ followed by an infinite\n",
    "sequence of 7s. What are $G_1$ and $G_0$ ?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c17a32a-291e-4582-baf5-ac8c9241655b",
   "metadata": {},
   "source": [
    "$G_0 = 2 + \\gamma G_1 = 2 + \\gamma \\frac{7}{1-\\gamma} = 2 + 0.9 \\cdot \\frac{7}{0.1} = 65$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1528b863-e2a0-4171-9db4-3cc64e1c21f3",
   "metadata": {
    "id": "7775e3be-eb05-48ba-b52a-8ddad01475b8"
   },
   "source": [
    "### 3.10\n",
    "Prove the second equality in (3.10).  \n",
    "$$\\sum_{k=0}^{\\infty}\\gamma^k = 1 + \\gamma + \\gamma^2 + ... = 1 + \\gamma(1 + \\gamma(1 + ...))$$\n",
    "$$ S_\\infty = a_1 + a_1 \\cdot q + a_1 \\cdot q^2 + ...$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1126a-da5f-4a03-bbb3-56afa0df17c4",
   "metadata": {},
   "source": [
    "This is just geometric series  \n",
    "$\\color{green}{(\\sum_{k=0}^\\infty y^k)(1-\\gamma)=\\sum_{k=0}^\\infty y^k(1-\\gamma) = \\sum_{k=0}^\\infty (\\gamma^k-\\gamma_{k+1}) = 1 - \\lim_{k\\rightarrow \\infty}\\gamma^{k+1}=1-0=1}$  \n",
    "Thus:  \n",
    "$\\color{green}{\\sum_{k=0}^\\infty \\gamma^k = \\frac{1}{1-\\gamma}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce133f-9da0-4f81-b155-74625dafe622",
   "metadata": {
    "id": "36ce133f-9da0-4f81-b155-74625dafe622"
   },
   "source": [
    "### 3.11\n",
    "\n",
    "If the current state is $S_t$ , and actions are selected according to a stochastic\n",
    "policy $\\pi$, then what is the expectation of $R_{t+1}$ in terms of $\\pi$ and the four-argument\n",
    "function $p$ (3.2)?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3679b5bf-bada-4ab3-b7c7-c7fab884b319",
   "metadata": {},
   "source": [
    "$ R_{t+1} = ùñ§_\\pi \\left[ \\sum_r r \\sum_{s'} p(s',r | s, a)\\right]$  \n",
    "$\\color{green}{\\sum_a \\pi(a|S_t)\\sum_{s',r}p(s',r|s,a)r}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d548cb7-6b88-4d68-9808-ed0c1dbf2633",
   "metadata": {
    "id": "1d548cb7-6b88-4d68-9808-ed0c1dbf2633"
   },
   "source": [
    "### 3.12\n",
    "Give an equation for $v_\\pi$ in terms of $q_\\pi$ and $\\pi$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f22caf-ee70-4cc3-8c94-264faf4219ad",
   "metadata": {},
   "source": [
    "$ v_{\\pi}(s) = \\sum_a\\color{green}{\\pi(a|s)}q_\\pi(s,a) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f379b3cd-dc89-4dc8-9ef2-a83ee59d0c5c",
   "metadata": {
    "id": "f379b3cd-dc89-4dc8-9ef2-a83ee59d0c5c"
   },
   "source": [
    "### 3.13\n",
    "Give an equation for $q_\\pi$ in terms of $v_\\pi$ and the four-argument $p$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9901fa45-b0f1-44dc-a0b9-4b2a53c0c8d4",
   "metadata": {},
   "source": [
    "$\\color{red}{q_\\pi(s,a) = v_\\pi(s) - \\sum_rr\\sum_{s'} p \\left(s',r|s,a \\right)}$  \n",
    "$\\color{green}{q_\\pi(s,a) = \\sum_{s',r}p \\left(s',r|s,a \\right) \\left[r+\\gamma v_\\pi(s') \\right]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416e1ec-c915-4d6c-9cc8-7addd37a06d5",
   "metadata": {
    "id": "6416e1ec-c915-4d6c-9cc8-7addd37a06d5"
   },
   "source": [
    "### 3.14\n",
    "The Bellman equation (3.14) must hold for each state for the value function\n",
    "$v_\\pi$ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds\n",
    "for the center state, valued at $+0.7$, with respect to its four neighboring states, valued at\n",
    "$+2.3$, $+0.4$, $0.4$, and $+0.7$. (These numbers are accurate only to one decimal place.)  \n",
    "![3_14 image](3_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94d732f-65db-466e-bd59-1a375f380d85",
   "metadata": {},
   "source": [
    "$\\frac{1}{4} \\cdot \\color{green}{0.9\\cdot}2.3 + \\frac{1}{4} \\cdot \\color{green}{0.9\\cdot} 0.7 + \\frac{1}{4} \\cdot \\color{green}{0.9\\cdot} 0.4 + \\frac{1}{4} \\cdot \\color{green}{0.9\\cdot} (-0.4) = \\color{red}{0.75}\\color{green}{0.675}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc31f4-c7e9-4c2a-806a-d1da503775e0",
   "metadata": {
    "id": "62dc31f4-c7e9-4c2a-806a-d1da503775e0"
   },
   "source": [
    "### 3.15\n",
    "In the gridworld example, rewards are positive for goals, negative for\n",
    "running into the edge of the world, and zero the rest of the time. Are the signs of these\n",
    "rewards important, or only the intervals between them? Prove, using (3.8), that adding a\n",
    "constant $c$ to all the rewards adds a constant, $v_c$ , to the values of all states, and thus\n",
    "does not affect the relative values of any states under any policies. What is $v_c$ in terms\n",
    "of $c$ and $\\gamma$?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb84bb02-60d3-41cc-808b-109679e5090f",
   "metadata": {},
   "source": [
    "$G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$  \n",
    "$v(s) = (G_t | S_t = s) = ùîº_\\pi[R_{t+1}+c+\\gamma G_{t+1}| S_t = s]$  \n",
    "$=\\sum_a \\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r + c + \\gamma ùîº_\\pi[G_{t+1}|S_{t+1}=s']] = $  \n",
    "$=\\sum_a \\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r + \\gamma ùîº_\\pi[G_{t+1}|S_{t+1}=s']] + \\frac{c}{1-\\gamma}$  \n",
    "$v_c = \\frac{c}{1-\\gamma}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sXvwuadEr8h6",
   "metadata": {
    "id": "sXvwuadEr8h6"
   },
   "source": [
    "### 3.16\n",
    "Now consider adding a constant $c$ to all the rewards in an episodic task,\n",
    "such as maze running. Would this have any effect, or would it leave the task unchanged\n",
    "as in the continuing task above? Why or why not? Give an example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bc4300-d1f5-4772-ab12-f1da17633070",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">It will definitely have an effect if we have immediate reward since we probably have big reward in the terminal step it will decrease it's leverage over smaller steps if we add other objectives, if not then it will leave the task unchanged.</span>  \n",
    "<span style=\"color:green\">The sign is critical because in an episodic task it gives more incentive to the agent to finish the task. Thus adding a constant C, if so changes the sign, would have an impact on how agent moves. Furthermore if it stays negative but absolute value is too little, it will give a wrong signal to the agent that the time of completing the job is not that important</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "luhBais4sHvP",
   "metadata": {
    "id": "luhBais4sHvP"
   },
   "source": [
    "### 3.17\n",
    "What is the Bellman equation for action values, that\n",
    "is, for $q_\\pi$? It must give the action value $q_\\pi(s, a)$ in terms of the action\n",
    "values, $q_(s'\n",
    ", a'\n",
    ")$, of possible successors to the state‚Äìaction pair $(s, a)$.\n",
    "Hint: The backup diagram to the right corresponds to this equation.\n",
    "Show the sequence of equations analogous to (3.14), but for action\n",
    "values.  \n",
    "  \n",
    "![3_17 image](3_17.png)  \n",
    "$q_\\pi = \\pi(a|s)\\sum_{s'r}p(s',r|s,a)[r + \\gamma v_\\pi(s')],$ for all $s \\in S$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e36e8de-3c24-4630-bb9e-e5a6ee393d0d",
   "metadata": {},
   "source": [
    "$\\color{red}{q_\\pi(s,a) = r(s,a) + \\sum_a \\pi(a|s) \\sum_rr\\sum_{s'}p(s',r|s,a)}$  \n",
    "$\\color{green}{q_\\pi(s,a) = \\sum_{s',r}p(s',r|s,a)[r + \\gamma \\sum_{a'}\\pi(a'|s')q_\\pi(s',a')]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vDXCaej7sc2Q",
   "metadata": {
    "id": "vDXCaej7sc2Q"
   },
   "source": [
    "### 3.18\n",
    "The value of a state depends on the values of the actions possible in that\n",
    "state and on how likely each action is to be taken under the current policy. We can\n",
    "think of this in terms of a small backup diagram rooted at the state and considering each\n",
    "possible action:  \n",
    "  \n",
    "![3_18 image](3_18.png)  \n",
    "  \n",
    "Give the equation corresponding to this intuition and diagram for the value at the root\n",
    "node, $v_\\pi(s)$, in terms of the value at the expected leaf node, $q_\\pi(s, a)$, given $S_t = s$. This\n",
    "equation should include an expectation conditioned on following the policy, $\\pi$. Then give\n",
    "a second equation in which the expected value is written out explicitly in terms of $\\pi(a|s)$\n",
    "such that no expected value notation appears in the equation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94f5edf-26cf-4e2d-b7ae-96b848f9d69f",
   "metadata": {},
   "source": [
    "$\\color{orange}{q_\\pi(s,a) = ùîº_\\pi \\left [G_{t} | S_t = s, A_t=a \\right] =\\sum_{s',r} p(s',r|s,a)[r+\\gamma \\sum_{a}q_\\pi(s',a)]}$  \n",
    "$\\color {green}{q_\\pi(s,a) = ùîº_\\pi \\left [q_\\pi(a|s) \\right ]= \\sum_a \\pi(a|s)q_\\pi(s,a)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saEy2owltkTA",
   "metadata": {
    "id": "saEy2owltkTA"
   },
   "source": [
    "### 3.19\n",
    "The value of an action, $q_\\pi(s, a)$, depends on the expected next reward and\n",
    "the expected sum of the remaining rewards. Again we can think of this in terms of a\n",
    "small backup diagram, this one rooted at an action (state‚Äìaction pair) and branching to\n",
    "the possible next states:  \n",
    "  \n",
    "![3_19 image](3_19.png)  \n",
    "  \n",
    "Give the equation corresponding to this intuition and diagram for the action value,\n",
    "$q_\\pi(s, a)$, in terms of the expected next reward, $R_{t+1}$, and the expected next state value,\n",
    "$v_\\pi(S_{t+1})$, given that $S_t = s$ and $A_t =a$. This equation should include an expectation but\n",
    "not one conditioned on following the policy. Then give a second equation, writing out the\n",
    "expected value explicitly in terms of $p(s'\n",
    ", r|s, a)$ defined by (3.2), such that no expected\n",
    "value notation appears in the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2a0f6-804c-4809-9b43-05557e4a503c",
   "metadata": {},
   "source": [
    "$\\color{red}{q_\\pi(s,a) = ùîº_\\pi[R_{t+1}+G_{t+1} | S_t=s, A_t=a] = R_{t+1} + \\gamma\\sum_rr\\sum_{s''}p(s'', r|s',a)}$  \n",
    "$\\color{green}{q_\\pi(s,a) = ùîº_\\pi[R_{t+1}+\\gamma v_\\pi(S_{t+1})| S_t=s, A_t=a] = \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_\\pi(s')]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_1L5KLy-uI8b",
   "metadata": {
    "id": "_1L5KLy-uI8b"
   },
   "source": [
    "### 3.20\n",
    "Draw or describe the optimal state-value function for the golf example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e16d067-037c-4035-a3e7-3572a1277a0d",
   "metadata": {},
   "source": [
    "$v_*(s_0) = -3$, then all subsequent will add -2 if correct action was choosen:  \n",
    "driver till green then putt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9xKCAUWkuNzH",
   "metadata": {
    "id": "9xKCAUWkuNzH"
   },
   "source": [
    "### 3.21\n",
    "Draw or describe the contours of the optimal action-value function for\n",
    "putting, $q_*(s, putter)$, for the golf example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ffed04-5c2d-4db7-8391-4b920ffd434d",
   "metadata": {},
   "source": [
    "It will be -4, since we will waste one shot compared to the optimal strategy,\n",
    "since after that we can use driver as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AYTDdzwouoTm",
   "metadata": {
    "id": "AYTDdzwouoTm"
   },
   "source": [
    "### 3.22\n",
    "Consider the continuing MDP shown to the\n",
    "right. The only decision to be made is that in the top state,\n",
    "where two actions are available, left and right. The numbers\n",
    "show the rewards that are received deterministically after\n",
    "each action. There are exactly two deterministic policies,\n",
    "$\\pi_{left}$ and $\\pi_{right}$. What policy is optimal if $\\gamma = 0$? If $\\gamma = 0.9$?\n",
    "If  $\\gamma = 0.5$?  \n",
    "  \n",
    "![3_22 image](3_22.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab0920e-b058-4cfc-b2c4-638c43fe5ce6",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "1) For $\\gamma=0$ the right action is better since the both take two steps to get back to the top state, and along the way we get more reward on the right\n",
    "2) For $\\gamma=0.9$ it's the same story, since $2 \\cdot 0.9^2 $ is still more that $0.9$  \n",
    "3) Here both policies are optimal, since $0.5 = 2 \\cdot 0.5^2$  \n",
    "</span>\n",
    "<p style=\"color:green\">  \n",
    "We should consider not immediate rewards, but farsighted approach!\n",
    "</p>\n",
    "  \n",
    "$\\color{green}{G_{\\pi_{left}} = \\sum_{i=0}^\\infty \\gamma^{2i}= \\frac{1}{1-\\gamma^2}}$  \n",
    "$\\color{green}{G_{\\pi_{right}} = \\sum_{i=0}^\\infty 2\\gamma^{1+2i} = \\frac{2\\gamma}{1-\\gamma^2}}$  \n",
    "<span style=\"color:green\">Based on that for $\\epsilon>0.5$ right is more optimal, for $\\epsilon<0.5$ left and for $\\epsilon==0.5$ both are optimal</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5v_YkXQIu-qt",
   "metadata": {
    "id": "5v_YkXQIu-qt"
   },
   "source": [
    "### 3.23\n",
    "Give the Bellman equation for $q_*$ for the recycling robot.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b6ebc-2adb-4a67-913d-789d2a93a4c9",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">$q_*(h, s) = argmax \\begin{cases} p(h,s,h) \\cdot r_{search} + {max}_{a}q(h,a) \\\\ p(l,s,h) \\cdot r_{search} + {max}_a q(l,a) \\end{cases} = argmax \\begin{cases} \\alpha \\cdot r_{search} + {max}_{a}q(h,a) \\\\ (1-\\alpha) \\cdot r_{search} + {max}_a q(l,a)\\end{cases}$</span>  \n",
    "<span style=\"color:green\">$q_*(h, s) = r_{wait} + \\gamma \\left(\\alpha max_a q(h,a) + (1-\\alpha) max_a q(l,a) \\right)$ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e481a-4287-44fe-8299-5357f0ed748e",
   "metadata": {},
   "source": [
    "$q_*(h, w)$ <span style=\"color:red\">$= argmax \\begin{cases} p(h,w,h) \\cdot r_{wait} + {max}_a q(h,a) \\\\ p(l,w,h) \\cdot r_{wait} + {max}_a q(l,a) \\end{cases}$</span> = $r_{wait} + \\gamma{max}_a q(h,a)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0536925-1edc-4705-921c-cc96f410541a",
   "metadata": {},
   "source": [
    "$q_*(l, w) $<span style=\"color:red\">\n",
    "$= argmax \\begin{cases} p(h,w,l) \\cdot r_{wait} + {max}_a q(h,a) \\\\ p(l,w,l) \\cdot r_{wait} + {max}_a q(l,a) \\end{cases}$\n",
    "</span>\n",
    "$=r_{wait} + {max}_a q(l,a)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51c3f72-0d05-4cac-8821-fc3c27fa88fc",
   "metadata": {},
   "source": [
    "$q_*(l, r) $\n",
    "<span style=\"color:red\">$= argmax \\begin{cases} p(h,r,l) \\cdot r_{recharge} + {max}_a q(h,a) \\\\ p(l,r,l) \\cdot r_{recharge}+ {max}_a q(l,a) \\end{cases}$\n",
    "</span>\n",
    "$= {max}_a q(h,a)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6227d-e7f0-44c6-8d32-01f10de85b29",
   "metadata": {},
   "source": [
    "$q_*(l, s) = \\beta(r_{search} + max_aq_*(l,a)) + (1-\\beta)(-3 + max_aq_*(h,a))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zOZL5UPnvDWW",
   "metadata": {
    "id": "zOZL5UPnvDWW"
   },
   "source": [
    "### 3.24\n",
    "Figure 3.5 gives the optimal value of the best state of the gridworld as\n",
    "24.4, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express\n",
    "this value symbolically, and then to compute it to three decimal places."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db9e68-bdd6-491a-9f23-a3bbf03c9536",
   "metadata": {},
   "source": [
    "$v_* = \\sum_k^\\infty\\gamma^{5k}10 \\approx 24.420$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WixN4lvkvM-y",
   "metadata": {
    "id": "WixN4lvkvM-y"
   },
   "source": [
    "### 3.25\n",
    "Give an equation for $v_*$ in terms of $q_*$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4ad641-7ede-48d3-aaa3-417ad1374add",
   "metadata": {},
   "source": [
    "$v_*(s) = {max}_a q_*(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WL02x95bvUEt",
   "metadata": {
    "id": "WL02x95bvUEt"
   },
   "source": [
    "### 3.26\n",
    "Give an equation for $q_*$ in terms of $v_*$ and the four-argument $p$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a66147-958c-4cf2-82e9-88af5066fe89",
   "metadata": {},
   "source": [
    "$\\color{red}{q_*(s,a) = {max}_{s'}p(s',r|s,a) + v_*(s')}$  \n",
    "$\\color{green}{q_*(s,a) = \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_*(s,a)]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BBP3AsnOvbxN",
   "metadata": {
    "id": "BBP3AsnOvbxN"
   },
   "source": [
    "### 3.27\n",
    "Give an equation for $\\pi_*$ in terms of $q_*$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a6f6a8-c1cf-49e3-af88-aeb7e89b2c3e",
   "metadata": {},
   "source": [
    "$\\pi_*(s,a) = \\begin{cases}1, ~~for ~every ~a ~for~which~q(s,a)=q_*(s,a)\\\\ 0,~~ for~every~other~case\\end{cases}$  \n",
    "$\\color{green}{a_* = arg \\pi_*(a_*|s)=arg {max}_a q_*(s,a)}$\n",
    "<span style=\"color:green\">Policies that map only these a ‚àó to their arbitrary possibilities would\n",
    "be the $œÄ_‚àó$.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EBzgrD98vhxp",
   "metadata": {
    "id": "EBzgrD98vhxp"
   },
   "source": [
    "### 3.28\n",
    "Give an equation for $\\pi_*$ in terms of $v_*$ and the four-argument $p$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b64cfcc-d4ff-49f8-a9bf-7194fd23fc48",
   "metadata": {},
   "source": [
    "$\\pi_*(s,a)=\\begin{cases}1,~~for~every~a~for~which~ a={max}_{a}p(s',r|s,a) v(s')\\\\0,~~for~every~other~case\\end{cases}$  \n",
    "  \n",
    "$\\color{green}{a_*=arg \\pi_*(a_*|s)=arg {max}_a\\sum_{s',r}p(s',r|s,a)[r + \\gamma v_*(s')]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zaMnND1zv551",
   "metadata": {
    "id": "zaMnND1zv551"
   },
   "source": [
    "### 3.29\n",
    "Rewrite the four Bellman equations for the four value functions ($v_\\pi, v_*, q_\\pi$,\n",
    "and $q_*)$ in terms of the three argument function p (3.4) and the two-argument function $r$\n",
    "(3.5).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7b7712-134a-4a75-80a7-7ca2db3c64e6",
   "metadata": {},
   "source": [
    "$v_\\pi = \\sum_a \\pi(a|s)p(s'|s,a) \\left[r(s,a)+\\gamma v_\\pi(s') \\right]_{(4.3)}$  \n",
    "$\\color{green}{v_\\pi(s)=ùîº_\\pi[G_t|s_t=s]=\\sum_a[r(s,a)+\\gamma \\sum_{s'}p(s'|s,a)v_\\pi(s')]\\pi(s,a)}$\n",
    "  \n",
    "$v_* = {max}_a p(s'|s,a)[r(s,a)+\\gamma v_*(s')]$  \n",
    "  \n",
    "  \n",
    "$q_\\pi = (s'|s,a)[r(s,a) + \\gamma \\sum_{a'}q_\\pi(s',a')]$  \n",
    "  \n",
    "$q_* = {max}_ar(s,a) + p(s'|s,a)[r(s,a) + \\gamma \\sum_{a'}q_\\pi(s',a')]$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeaa950-90c3-4658-aab9-7ad06fe21cc1",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">$v_\\pi = \\sum_a \\pi(a|s) \\sum_{s'} p(s,a,s')(r(s,a) + \\gamma v_\\pi(s'))$</span>  \n",
    "<span style=\"color:green\">$v_\\pi(s) = \\sum_a \\pi(a|s) \\left[ r(s,a) + \\gamma\\sum_{s'}p(s,a,s') v_\\pi(s')\\right]$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a76ed3-1fd5-4b58-9e2f-e3354a86ddb5",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow\">$v_*(s) = max_a[r(s,a) + \\gamma \\sum_{s'}p(s,a,s')v_*(s')]$</span>  \n",
    "<span style=\"color:green\">$v_*(s) = \\sum_a \\pi_*(a|s) \\left[ r(s,a) + \\gamma\\sum_{s'}p(s,a,s') v_*(s')\\right]$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751663a-b67a-4348-b4b2-60a1e757059f",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">$q_\\pi(s,a) = r(s,a) + \\gamma \\sum_{s',a'}p(s,a,s')q(s',a')$</span>  \n",
    "<span style=\"color:green\">$q_\\pi(s,a) = r(s,a) + \\gamma \\sum_{s'}p(s,a,s')\\sum_{a'}\\pi(a'|s')q(s',a')$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6db7e19-a71a-427f-8c89-2e04f08665d3",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">$q_*(s,a) = r(s,a) + \\gamma\\sum_{s'}p(s,a,s')max_aq(s',a')$</span>  \n",
    "<span style=\"color:green\">$q_*(s,a) = r(s,a) + \\gamma\\sum_{s'}p(s,a,s')\\sum_{a'}\\pi_*(a'|s')q_*(s',a')$</span>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Chapter 3 Finnite Markov Decision Processes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
