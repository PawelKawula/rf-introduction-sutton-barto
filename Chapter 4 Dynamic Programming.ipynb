{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e65638-cb63-496e-9c49-fb9b34eadc20",
   "metadata": {},
   "source": [
    "### 4.1\n",
    "In Example 4.1, if $\\pi$ is the equiprobable random policy, what is $q_\\pi(11, down)$?\n",
    "What is $q_\\pi (7, down)$?  \n",
    "![4_1 image](4_1.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529c7dd-438f-45f8-9fb1-46828077c6be",
   "metadata": {},
   "source": [
    "$q_\\pi(11,down) = -1$  \n",
    "$q_\\pi(7,down) = \\sum_{s',r}p(s', r| s,a)(r+\\gamma\\cdot q_\\pi(s',a')) = $  \n",
    "$p(11, -1 | 7, down)(-1 + 0.9 \\cdot v_\\pi(11)) = -1 + 0.9 \\cdot v_\\pi(11)$  \n",
    "$v_\\pi(11) = \\sum_{a}\\frac{1}{4}q_\\pi(11,a) = \\frac{1}{4}q_\\pi(11, down) + \\frac{1}{4}q_\\pi(11, left) + \\frac{1}{4}q_\\pi(11, up) + 0.9\\frac{1}{4}v_\\pi(11)$  \n",
    "  \n",
    "Actually we can just compute it using $v_\\pi(11): q_\\pi(7, down) = -1 + \\gamma v_\\pi(11) = -1 + 0.9 \\cdot (-14) = -15$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1a8d8-5e86-432a-b56f-f1e2590746a6",
   "metadata": {},
   "source": [
    "### 4.2\n",
    "In Example 4.1, suppose a new state 15 is added to the gridworld just below\n",
    "state $13$, and its actions, $left, up, right$, and $down$, take the agent to states $12, 13, 14,$\n",
    "and $15$, respectively. Assume that the transitions from the original states are unchanged.\n",
    "What, then, is $v_\\pi(15)$ for the equiprobable random policy? Now suppose the dynamics of\n",
    "state $13$ are also changed, such that action down from state $13$ takes the agent to the new\n",
    "state $15$. What is $v_\\pi(15)$ for the equiprobable random policy in this case?  \n",
    "![4_1_1 image](4_1_1.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ec7dc-cd57-46bb-8dd1-e78be8566607",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "For original states unchanged:\n",
    "$v_\\pi(15) = \\gamma \\frac{1}{4}(-22.0) + \\gamma \\frac{1}{4}(-20.0) + \\gamma \\frac{1}{4}(-14.0) + \\frac{1}{4}$\n",
    "$\\gamma v_\\pi(15) - 1$  \n",
    "$0.76 v_\\pi(15) = -13.6 - 1=-19.21$  \n",
    "For state 13 changed:  \n",
    "$v_\\pi(13) = \\gamma \\frac{1}{4}(-22.0) + \\gamma \\frac{1}{4}(-20.0) + \\gamma \\frac{1}{4}(-14.0)$\n",
    "$+ \\gamma \\frac{1}{4}v_\\pi(15) - 1$  \n",
    "$0.76v_\\pi(13) = -13.6 + 0.225 v_\\pi(15) \\leftrightarrow v_\\pi(13) = -17.9 + 0.3 v_\\pi(15)$  \n",
    "$v_\\pi(15) = \\gamma \\frac{1}{4}(-22.0) + \\gamma \\frac{1}{4}v_\\pi(13) + \\gamma \\frac{1}{4}(-14.0)$\n",
    "$+ \\gamma \\frac{1}{4}v_\\pi(15) -1$  \n",
    "$0.76 v_\\pi(15) = -9.1 + 0.225 v_\\pi(13) \\leftrightarrow 0.76v_\\pi(15) = -9.1 - 4 + 0.07 v_\\pi(15)$\n",
    "$\\leftrightarrow 0.69 v_\\pi(15) = -13.1 = -19$\n",
    "$v_\\pi(13) = -16.6 - 0.3 \\cdot 19 = -21.7$  \n",
    "</span>  \n",
    "<p style=\"color:green\">Changing the dynamics should not result in recalculation since set of S' for S=15 \n",
    "is exactly as the one of S=13, thus they must share this value</p>  \n",
    "<p style=\"color:orange\">Maybe it's wrong cause i use discounting factor?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ece14-5285-4e00-80fe-67be96c833c2",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">\n",
    "$v_\\pi(15) = \\sum_{s',r}p(s',r|s,a)\\left[r + v_\\pi(s')\\right] =$\n",
    "$-1 + \\frac14(-22-20-14+v_\\pi(15)) = -15 + \\frac14v_\\pi(15)$<br>\n",
    "$v_\\pi(15) = -15 + \\frac14v_\\pi(15) \\leftrightarrow v_\\pi(15) = -20$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4bd1a6-9e23-42a6-9eb2-3c191b6f1ce7",
   "metadata": {},
   "source": [
    "### 4.3\n",
    "What are the equations analogous to (4.3), (4.4), and (4.5), but for *action*-value functions instead of state-value functions?  \n",
    "$v_\\pi(s) = E_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t = s]~~~(4.3)$  \n",
    "$v_\\pi(s) = \\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+ \\gamma v_\\pi(s')]~~(4.4)$  \n",
    "$v_{k+1}(s) = \\sum_a \\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_k(s')]~~(4.5)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61866b32-ac51-45a4-bbb0-34e354a54209",
   "metadata": {},
   "source": [
    "$q(s,a) = E_\\pi[R_{t+1} + \\gamma \\sum_{s',a'}q_\\pi(s', a') | S_t = s, A_t = a]$  \n",
    "$q_\\pi(s,a) = \\sum_{s',r}p(s',r|s,a)[r+\\gamma \\sum_{a'}\\pi(a'|s')q_\\pi(s', a')]$  \n",
    "$q_{k+1}(s,a) = \\sum_{s',r}p(s',r|s,a)[r + \\gamma \\sum_{a'}\\pi(a'|s')q_k(s',a')]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45a5bfb-01b0-497a-97a1-9dfba7f5f9c9",
   "metadata": {},
   "source": [
    "### 4.4\n",
    "The policy iteration algorithm on page 80 has a subtle bug in that it may\n",
    "never terminate if the policy continually switches between two or more policies that are\n",
    "equally good. This is okay for pedagogy, but not for actual use. Modify the pseudocode\n",
    "so that convergence is guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237bd95d-7b9b-4ece-ad25-16c3f10ec1e3",
   "metadata": {},
   "source": [
    "In policy improvement:  \n",
    "if $v(s,{argmax}_a(...)) > v_\\pi(s)$ then *policy-stable* $\\leftarrow$ *false*  \n",
    "  \n",
    "<span style=\"color:green\">Probably correct but this one was in aswer:<br>If old-action $\\notin \\lbrace $\n",
    "$a_i \\rbrace$, which is all the equi-best solutions from $\\pi(s)$,...</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3394077-361a-4f07-a03e-b5682e593b14",
   "metadata": {},
   "source": [
    "### 4.5\n",
    "Exercise 4.5 How would policy iteration be defined for action values? Give a complete\n",
    "algorithm for computing $q_\\pi$ , analogous to that on page $80$ for computing $v_\\pi$ . Please pay\n",
    "special attention to this exercise, because the ideas involved will be used throughout the\n",
    "rest of the book."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306eede-9272-4151-a2c2-263b76d800db",
   "metadata": {},
   "source": [
    "1. Initialization  \n",
    "$Q(s,a) \\in \\Re$ and $\\pi(s) \\in A(s)~arbitrarily~for~all~s\\in S; Q(terminal, a \\in A(s)) = 0$  \n",
    "2. Policy Iteration  \n",
    "Do:  \n",
    "&emsp;Loop:  \n",
    "&emsp;&emsp;$\\Delta \\leftarrow 0$, *policy-stable* $\\leftarrow$ *true*  \n",
    "&emsp;&emsp;Loop for each $s \\in S$:  \n",
    "&emsp;&emsp;&emsp;*old-action* = $\\pi(s)$  \n",
    "&emsp;&emsp;&emsp;Loop for each $a \\in A(s)$:  \n",
    "&emsp;&emsp;&emsp;&emsp;$Q(s,a) \\leftarrow \\sum_{s',r}p(s',r|s,a)[r+\\gamma Q(s',\\pi(s')]$  \n",
    "&emsp;&emsp;&emsp;$\\pi(s) = {argmax}_a Q(s,a)$  \n",
    "&emsp;&emsp;&emsp;if *old-action* $\\ne \\pi(s)$ then *policy-stable* $\\leftarrow$ *false*  \n",
    "&emsp;&emsp;$\\Delta \\leftarrow max(\\Delta, |q-Q(s,a)|)$  \n",
    "&emsp;until $\\Delta< \\theta$  \n",
    "While *policy-stable* = *false*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08060fc5-3f85-4513-af0c-426c6b497637",
   "metadata": {},
   "source": [
    "1. Initialization  \n",
    "$Q(s,a) \\in \\Re$ and $\\pi(s) \\in A(s)~arbitrarily~for~all~s\\in S; Q(terminal, a \\in A(s)) = 0$  \n",
    "1. Policy Evaluation \n",
    "Do:  \n",
    "&emsp;Loop:  \n",
    "&emsp;&emsp;$\\Delta \\leftarrow 0$  \n",
    "&emsp;&emsp;Loop for each $s \\in S$:  \n",
    "&emsp;&emsp;&emsp;Loop for each $a \\in A(s)$:  \n",
    "&emsp;&emsp;&emsp;&emsp;$q \\leftarrow Q(s,a)$  \n",
    "&emsp;&emsp;&emsp;&emsp;$Q(s,a) \\leftarrow \\sum_{s',r}p(s',r|s,a)[r+\\gamma \\color{orange}{Q(s',\\pi(s')]}$\n",
    "$\\color{green}{\\sum_a'}$\n",
    "$\\color{green}{\\pi(a',s')Q(s',a')}$  \n",
    "&emsp;&emsp;&emsp;$\\Delta \\leftarrow max(\\Delta, |q-Q(s,a)|)$  \n",
    "&emsp;until $\\Delta< \\theta$  \n",
    "3. Policy Improvement   \n",
    "&emsp;&emsp;*policy-stable* $\\leftarrow$ *true*  \n",
    "&emsp;&emsp;Loop for each $s \\in S, a \\in A$  \n",
    "&emsp;&emsp;&emsp;*old-action* = $\\pi(s)$  \n",
    "&emsp;&emsp;&emsp;$\\pi(s) = {argmax}_a Q(s,a)$  \n",
    "<span style=\"color:orange\">&emsp;&emsp;&emsp;if $v(s,{argmax}_a(...)) > v_\\pi(s)$ then *policy-stable* $\\leftarrow$ *false*</span>  \n",
    "<span style=\"color:green\">&emsp;&emsp;&emsp;if *old-action* $\\ne \\{ a_i\\}$, which is set of all the equi-best solutions from $\\pi(s)$ then *policy-stable* $\\leftarrow$ *false*</span>  \n",
    "&emsp;&emsp;While *policy-stable* = *false*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd3e94b-622f-47ea-be06-29d071a63c52",
   "metadata": {},
   "source": [
    "### 4.6\n",
    "Suppose you are restricted to considering only policies that are $\\epsilon$-*soft*,\n",
    "meaning that the probability of selecting each action in each state, $s$, is at least $\\frac{\\epsilon}{|A(s)|}$.\n",
    "Describe qualitatively the changes that would be required in each of the steps $3, 2$, and $1$,\n",
    "in that order, of the policy iteration algorithm for $v_*$ on page 80."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5182bce6-dde7-4c24-92d4-fb877c0a05e6",
   "metadata": {},
   "source": [
    "It would first change that $\\pi(s)$ would be either argmax $\\epsilon * 100$ % of times or else random action  \n",
    "Then idk\n",
    "\n",
    "<span style=\"color:green\">Step 3: We will only decide if policy is not stable under condition that policy\n",
    "does not explore<br><br>Step 2: $\\theta$ should not be set above the limit of any $soft-\\epsilon$ method<br><br>\n",
    "Step 1: $\\pi$ should be well defined as $soft-\\epsilon$ method. $\\epsilon$ should be given</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d57f5-93fa-44c4-ab9f-701651f10116",
   "metadata": {},
   "source": [
    "### 4.7\n",
    "(programming) Write a program for policy iteration and re-solve Jack’s car\n",
    "rental problem with the following changes. One of Jack’s employees at the first location\n",
    "rides a bus home each night and lives near the second location. She is happy to shuttle\n",
    "one car to the second location for free. Each additional car still costs 2\\\\$, as do all cars\n",
    "moved in the other direction. In addition, Jack has limited parking space at each location.\n",
    "If more than 10 cars are kept overnight at a location (after any moving of cars), then an\n",
    "additional cost of 4\\\\$ must be incurred to use a second parking lot (independent of how\n",
    "many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often\n",
    "occur in real problems and cannot easily be handled by optimization methods other than\n",
    "dynamic programming. To check your program, first replicate the results given for the\n",
    "original problem.  \n",
    "![4_2 image](4_2.png)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bba8bd72-651b-4305-9e29-668610c9b590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 policy iteration:\n",
      ":(\n",
      "2 policy iteration:\n",
      ":(\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import poisson, randint\n",
    "from numpy import zeros, argmax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_spots = 5\n",
    "l_first_rent, l_second_rent = 3, 4\n",
    "l_first_ret, l_second_ret = 3, 2\n",
    "pi = randint(-5, 6, (n_spots + 1, n_spots + 1))\n",
    "gm, th = 0.9, 0.01\n",
    "Vs = zeros((n_spots + 1, n_spots + 1))\n",
    "p_iter, p_eval = 0, 0\n",
    "\n",
    "#policy evaluation\n",
    "while True:\n",
    "    p_iter, p_eval = p_iter+1, 0\n",
    "    print(f\"{p_iter} policy iteration:\")\n",
    "    while True:\n",
    "        dlt = 0\n",
    "        for i in range(len(Vs)):\n",
    "            for j in range(len(Vs[i])):\n",
    "                v = Vs[i][j]\n",
    "                f_rent, s_rent = poisson(l_first_rent), poisson(l_second_rent)\n",
    "                f_ret, s_ret = poisson(l_first_ret), poisson(l_second_ret)\n",
    "                spl_x = max(min(n_spots, i - f_rent + f_ret - pi[i][j]), 0)\n",
    "                spl_y = max(min(n_spots, j - s_rent + s_ret + pi[i][j]), 0)\n",
    "                Vs[i][j] = 10 * (min(f_rent, i) + min(j,s_rent)) - 2 * abs(pi[i][j]) + gm * Vs[spl_x][spl_y]\n",
    "                #print(f\"Before on spots: {i, j}; Rented cars: {f_rent, s_rent}, returned: {f_ret, s_ret}, moved: {pi[i][j]} now on spot: {spl_x, spl_y} reward: {Vs[i][j]}\")\n",
    "                dlt = max(dlt, abs(v - Vs[i][j]))\n",
    "        if dlt < th or p_eval > 2000: break\n",
    "        p_eval += 1\n",
    "    if p_eval > 20: print(\":(\")\n",
    "    for i in range(len(pi)):\n",
    "        for j in range(len(pi[i])):\n",
    "            old_action = pi[i][j]\n",
    "            move, m_act = -5, 0\n",
    "            while move < 6:\n",
    "                spl_x, spl_y = max(min(n_spots, i - move), 0), max(min(n_spots, j + move), 0)\n",
    "                i_max, j_max = max(min(n_spots, i - m_act), 0), max(min(n_spots, j + m_act), 0)\n",
    "                m_act = move if Vs[i_max][j_max] < Vs[spl_x][spl_y] else m_act\n",
    "                move += 1\n",
    "            if m_act != pi[i][j]:\n",
    "                policy_stable = False\n",
    "    if policy_stable or p_iter > 1: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b48c779c-0e65-47b2-9de8-b40ac3fd935d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301.4812517771154"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(v) for v in Vs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ee4e2-a9d3-44b3-ba77-529b3b76d0b5",
   "metadata": {},
   "source": [
    "### 4.8\n",
    "Why does the optimal\n",
    "policy for the gambler’s problem have such a curious form? In particular, for capital of 50\n",
    "it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f91c3-19c6-4922-8564-6c9e1c184803",
   "metadata": {},
   "source": [
    "It's because we have chances for one bet smaller than 0.5 and it will compound if we try to bet multiple times, so when we have smallest amount to win in fe. 3 bets we take the smallest bet to win in these.\n",
    "\n",
    "<span style=\"color:green\">In addition we can try to bet this excessive 1 dolar to get to the next level and if not then we just bet all</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e68dd-cc82-49bf-9a8c-f4356fc077d3",
   "metadata": {},
   "source": [
    "### 4.9\n",
    "(programming) Implement value iteration for the gambler’s problem and\n",
    "solve it for $p_h = 0.25$ and $p_h = 0.55$. In programming, you may find it convenient to\n",
    "introduce two dummy states corresponding to termination with capital of 0 and 100,\n",
    "giving them values of 0 and 1 respectively. Show your results graphically, as in Figure 4.3.\n",
    "Are your results stable as $\\theta \\rightarrow 0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1053c9b-996e-44a7-8963-d6eb9d57fc45",
   "metadata": {},
   "source": [
    "### 4.10\n",
    "What is the analog of the value iteration update (4.10) for action values,\n",
    "$q_{k+1}(s, a)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe8c4f-77e7-4771-8d73-fd1eaa4a792e",
   "metadata": {},
   "source": [
    "$q_{k+1}(s,a) = \\sum_{s',r}p(s',r|s,a)[r + \\gamma {max}_{a'}q_k(s', a')]$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
