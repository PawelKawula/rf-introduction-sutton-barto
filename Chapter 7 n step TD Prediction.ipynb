{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce3c42d-b46f-4d60-96b8-4b562787d67c",
   "metadata": {},
   "source": [
    "### 7.1\n",
    "In Chapter 6 we noted that the Monte Carlo error can be written as the\n",
    "sum of $TD$ errors (6.6) if the value estimates don’t change from step to step. Show that\n",
    "the *n*-step error used in (7.2) can also be written as a sum $TD$ errors (again if the value\n",
    "estimates don’t change) generalizing the earlier result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504552ed-b97f-4ee2-abad-1f8b7d06c790",
   "metadata": {},
   "source": [
    "$G_{t:t+n} - V(S_t) = R_{t+1} + G_{t+1:t+n} - V_t(S_t) = R_{t+1} + G_{t+1:t+n} - V_t(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea738141-4c0a-4e44-9499-a217a25be356",
   "metadata": {},
   "source": [
    "But because we assume that the value est. don't change: $V_{t+n} = V_t$ for all $t, n \\in \\mathbb{N}_+$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3805f89-154e-473c-8d2f-46c5a15d114d",
   "metadata": {},
   "source": [
    "&emsp;$=\\delta_t + \\gamma(G_{t+1} - V(S_{t+1})) = \\delta_t + \\gamma\\delta_{t+1} + \\gamma^2(G_t - V(S_{t+2}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fbc19f-b595-4895-8475-decaa1bec7b0",
   "metadata": {},
   "source": [
    "&emsp;$=\\delta_t + \\gamma\\delta_{t+1} + ... + \\gamma^{T-t-1}\\delta_{T-1} + \\gamma^{T-t}(G_T - V(S_T))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6791ae-69d4-4e8c-a10c-95b901d20ca0",
   "metadata": {},
   "source": [
    "&emsp;$=\\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2583d34c-fb23-4d02-a21e-a30283322e42",
   "metadata": {},
   "source": [
    "It's state-action pair value functions not state values but idc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef8a53-0a9b-404a-bbd3-7297ed9a539c",
   "metadata": {},
   "source": [
    "### 7.2\n",
    "(programming) With an *n*-step method, the value estimates do change from\n",
    "step to step, so an algorithm that used the sum of $TD$ errors (see previous exercise) in\n",
    "place of the error in (7.2) would actually be a slightly different algorithm. Would it be a\n",
    "better algorithm or a worse one? Devise and program a small experiment to answer this\n",
    "question empirically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3d9a98-f27d-464b-ba6e-9872eb7c075c",
   "metadata": {},
   "source": [
    "### 7.3\n",
    "Why do you think a larger random walk task ($19$ states instead of $5$) was\n",
    "used in the examples of this chapter? Would a smaller walk have shifted the advantage\n",
    "to a different value of *n*? How about the change in left-side outcome from $0$ to $1$ made\n",
    "in the larger walk? Do you think that made any difference in the best value of *n*?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7272b27-3b1b-46bf-835c-1e0580a2429f",
   "metadata": {},
   "source": [
    "Smaller walk would require less predicting of the future so we will not benefit from\n",
    "larger step, and i think it would shift the advantage to n=2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902fbd0c-bb0f-4be4-be64-b3011de19b81",
   "metadata": {},
   "source": [
    "### 7.4\n",
    "Prove that the n-step return of Sarsa (7.4) can be written exactly in terms\n",
    "of a novel TD error, as\n",
    "$$G_{t:t+n} = Q_{t-1}(S_t,A_t) + \\sum_{k=t}^{min(t+n,T)-1} \\gamma^{k-t}[R_{k+1}+\\gamma Q_k(S_{k+1},A_{k+1})-Q_{k-1}(S_k,A_k)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b89a67-5ce1-4b4e-92ef-f28e550a3022",
   "metadata": {},
   "source": [
    "$G_{t+t+n} = \\sum_{k=t}^{t+n}\\gamma^{k-t}R_{k+1} + \\gamma^nQ_{t+n-1}(S_{t+n}, A_{t+n})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55b755c-e38b-4c90-9879-91d158b5f3a8",
   "metadata": {},
   "source": [
    "$\\sum_{k=t}^{t+n}\\gamma^{k-t}R_{k+1} + \\gamma^nQ_{t+n-1}(S_{t+n}, A_{t+n})$  \n",
    "$=$  \n",
    "$Q_{t-1}(S_t,A_t) + \\sum_{k=t}^{min(t+n,T)-1} \\gamma^{k-t}[R_{k+1}+\\gamma Q_k(S_{k+1},A_{k+1})-Q_{k-1}(S_k,A_k)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ef0a89-85c3-4b1b-a90c-0ea55a7149e5",
   "metadata": {},
   "source": [
    "$\\gamma^nQ_{t+n-1}(S_{t+n}, A_{t+n}) = Q_{t-1}(S_t,A_t) + \\sum_{k=t}^{min(t+n,T)-1} \\gamma^{k-t}[\\gamma Q_k(S_{k+1},A_{k+1})-Q_{k-1}(S_k,A_k)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e1639-d8c4-4aea-a1bc-b6569a5f5005",
   "metadata": {},
   "source": [
    "$Q_{t+n-1}(S_{t+n}, A_{t+n}) = \\frac{Q_{t-1}(S_t,A_t)}{\\gamma^n} + \\sum_{k=t}^{min(t+n,T)-1} \\gamma^{k-n-t}[\\gamma Q_k(S_{k+1},A_{k+1})-Q_{k-1}(S_k,A_k)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef03b3e-8207-4ce5-937f-16ad8906d4a4",
   "metadata": {},
   "source": [
    "IDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb86d7-f0dc-4c0b-8638-d4256b51c2d2",
   "metadata": {},
   "source": [
    "### 7.5 (hard)\n",
    "Write the pseudocode for the off-policy state-value prediction algorithm\n",
    "described above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68761e3a-b530-459a-8e79-dc8dd892c732",
   "metadata": {},
   "source": [
    "### 7.6\n",
    "Prove that the control variate in the above equations does not change the\n",
    "expected value of the return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be6a40-e782-4476-afcd-724afb37d876",
   "metadata": {},
   "source": [
    "### 7.7\n",
    "Write the pseudocode for the off-policy action-value prediction algorithm\n",
    "described immediately above. Pay particular attention to the termination conditions for\n",
    "the recursion upon hitting the horizon or the end of episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc097b8-a288-4183-96b0-74a9e7f2e792",
   "metadata": {},
   "source": [
    "### 7.8\n",
    "Show that the general (off-policy) version of the *n*-step return (7.13) can\n",
    "still be written exactly and compactly as the sum of state-based $TD$ errors (6.5) if the\n",
    "approximate state value function does not change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1f870-a9f4-41a6-85d9-e033d545516d",
   "metadata": {},
   "source": [
    "### 7.9\n",
    "Repeat the above exercise for the action version of the off-policy *n*-step\n",
    "return (7.14) and the Expected Sarsa $TD$ error (the quantity in brackets in Equation 6.9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d94396-c234-4fea-9e3e-21f5161eda58",
   "metadata": {},
   "source": [
    "### 7.10\n",
    "(programming) Devise a small off-policy prediction problem and use it to\n",
    "show that the off-policy learning algorithm using (7.13) and (7.2) is more data efficient\n",
    "than the simpler algorithm using (7.1) and (7.9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21f313-989c-422a-b0bb-992163757941",
   "metadata": {},
   "source": [
    "### 7.11\n",
    "Show that if the approximate action values are unchanging, then the\n",
    "tree-backup return (7.16) can be written as a sum of expectation-based TD errors:\n",
    "$$G_{t:t+n}=Q(S_t, A_t) + \\sum_{k=t}^{min(t+n-1,T-1)} \\delta_k \\prod_{i=t+1}^k\\gamma\\pi(A_i|S_i)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
