{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628505ea-db81-4232-b1db-71625a787677",
   "metadata": {},
   "source": [
    "### 6.1\n",
    "If $V$ changes during the episode, then (6.6) only holds approximately; what\n",
    "would the difference be between the two sides? Let $V_t$ denote the array of state values\n",
    "used at time $t$ in the TD error (6.5) and in the TD update (6.2). Redo the derivation\n",
    "above to determine the additional amount that must be added to the sum of TD errors\n",
    "in order to equal the Monte Carlo error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bdc9e6-a650-4c90-8b0d-3760cdb36f68",
   "metadata": {},
   "source": [
    "$G_t - V(S_t) = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) + \\gamma V(S_{t+1}) - \\gamma V(S_{t+1})$  \n",
    "&emsp;$=\\sum_{k=1}^{T-1}\\gamma^{k-t}\\delta_k$ (6.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091d7f9a-f5a1-4770-aa9d-c080365e0151",
   "metadata": {},
   "source": [
    "$\\sum_{k=1}^{T-1}\\gamma^{k-t}\\delta_k - V_t + V_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43283ac-930e-44df-98b1-9044ffc818be",
   "metadata": {},
   "source": [
    "### 6.2\n",
    "This is an exercise to help develop your intuition about why TD methods\n",
    "are often more efficient than Monte Carlo methods. Consider the driving home example\n",
    "and how it is addressed by TD and Monte Carlo methods. Can you imagine a scenario\n",
    "in which a TD update would be better on average than a Monte Carlo update? Give\n",
    "an example scenario—a description of past experience and a current state—in which\n",
    "you would expect the TD update to be better. Here’s a hint: Suppose you have lots\n",
    "of experience driving home from work. Then you move to a new building and a new\n",
    "parking lot (but you still enter the highway at the same place). Now you are starting\n",
    "to learn predictions for the new building. Can you see why TD updates are likely to be\n",
    "much better, at least initially, in this case? Might the same sort of thing happen in the\n",
    "original scenario?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92135456-884d-4a08-ad9a-d47efef1886a",
   "metadata": {},
   "source": [
    "It should be better in change that our enironment \n",
    "has a change because it will anticipate \n",
    "this change quicker and it will see in which\n",
    "state did it occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553644f6-c0bf-4b68-b3e7-1a7a53eb1733",
   "metadata": {},
   "source": [
    "### 6.3\n",
    "From the results shown in the left graph of the random walk example it\n",
    "appears that the first episode results in a change in only $V(A)$. What does this tell you\n",
    "about what happened on the first episode? Why was only the estimate for this one state\n",
    "changed? By exactly how much was it changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f808c177-bf75-466b-b702-c2ba4d37c767",
   "metadata": {},
   "source": [
    "The first episode ended with going straight to left  \n",
    "$\\alpha$ in the first episode $V(A)$ when we computed\n",
    "$V(B)$ was still 0.5,  \n",
    "so the change was non existing,\n",
    "since this task was undiscounted:  \n",
    "&emsp;$V(B) = 0.5 + \\alpha(0 + \\gamma 0.5 - 0.5) = 0.5 + 0.1(0) = 0.5$  \n",
    "&emsp;$V(A) = 0.5 + \\alpha(0 + \\gamma 0 - 0.5) = 0.45$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce16e8-3b2b-4302-a68d-957556ff682d",
   "metadata": {},
   "source": [
    "### 6.4\n",
    "The specific results shown in the right graph of the random walk example\n",
    "are dependent on the value of the step-size parameter, $\\alpha$. Do you think the conclusions\n",
    "about which algorithm is better would be affected if a wider range of $\\alpha$ values were used?\n",
    "Is there a different, fixed value of $\\alpha$ at which either algorithm would have performed\n",
    "significantly better than shown? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8792b077-3793-44c5-aa66-250c4471e819",
   "metadata": {},
   "source": [
    "I guess if $\\alpha$ would be $\\frac{1}{6k}$, where k $\\in \\mathbb{N}_+$\n",
    "then our method could stumble upon the best values, since it wouldn't\n",
    "jump over them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4515f3-0c2c-4dde-86a1-adddfb697817",
   "metadata": {},
   "source": [
    "### 6.5 (hard)\n",
    "In the right graph of the random walk example, the $RMS$ error of the\n",
    "$TD$ method seems to go down and then up again, particularly at high $\\alpha$’s. What could\n",
    "have caused this? Do you think this always occurs, or might it be a function of how the\n",
    "approximate value function was initialized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab86996-082a-46a3-b46d-16c768eeaa14",
   "metadata": {},
   "source": [
    "### 6.6\n",
    "In Example 6.2 we stated that the true values for the random walk example\n",
    "are $\\frac{1}{6}$ , $\\frac{2}{6}$ , $\\frac3 6$ , $\\frac46$ , and $\\frac56$ , for states `A` through `E`. Describe at least two different ways that\n",
    "these could have been computed. Which would you guess we actually used? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3272fa5-b6a2-4164-8032-ddff54b443e9",
   "metadata": {},
   "source": [
    "Center state should have probability of 0.5,\n",
    "then we can compute from TD these values from E through A\n",
    "&emsp;$V(E) = 0.5 \\cdot 1 + 0.5 V(D) = 0.5 + 0.5V(D)$  \n",
    "&emsp;$V(D) = 0.5 \\cdot V(E) + 0.5 \\cdot 0.5 = 0.25 + 0.5V(E)$  \n",
    "&emsp;$V(B) = 0.5 \\cdot V(A) + 0.5 \\cdot 0.5 = 0.25 + 0.5V(A)$  \n",
    "&emsp;$V(A) = 0.5 \\cdot V(B) + 0.5 \\cdot 0.0 = 0.5V(B)$  \n",
    "  \n",
    "&emsp;$V(B) = 0.25 + 0.25V(B) \\leftrightarrow 0.75V(B) = 0.25$\n",
    "&emsp;$\\leftrightarrow V(B) = \\frac13$  \n",
    "&emsp;$V(A) = \\frac16$  \n",
    "&emsp;$V(E) = 0.5 + 0.25V(E) + 0.125 \\leftrightarrow 0.75V(B) = 0.625$\n",
    "&emsp;$\\leftrightarrow V(E) = \\frac56$  \n",
    "&emsp;$V(D) = \\frac23$  \n",
    "  \n",
    "It could also be computed using probability that\n",
    "moves to one side are fe. +3 to the other etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7800c1ee-eaaf-4a67-8808-34e96a205cce",
   "metadata": {},
   "source": [
    "### 6.7 (hard)\n",
    "Design an off-policy version of the $TD(0)$ update that can be used with\n",
    "arbitrary target policy $\\pi$ and covering behavior policy $b$, using at each step $t$ the importance\n",
    "sampling ratio $p_{t:t}$ (5.3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8659f3-f39a-4d10-82dc-450123a045b0",
   "metadata": {},
   "source": [
    "### 6.8\n",
    "Show that an action-value version of (6.6) holds for the action-value form\n",
    "of the TD error $\\delta_t= R_{t+1} + \\gamma Q(S_{t+1} , A_{t+1} ) - Q(S_t , A_t )$, again assuming that the values\n",
    "don’t change from step to step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294832d1-ab02-4929-a6e1-89fb52a1b33e",
   "metadata": {},
   "source": [
    "&emsp;$G_t - V(S_t) = R_{t+1} + \\gamma G_{t+1} - Q(S_t)$\n",
    "$+ \\gamma Q(S_{t+1}, A_{t+1}) - \\gamma Q(S_{t+1}, A_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb428c6-7953-43f5-835a-12348ce62fff",
   "metadata": {},
   "source": [
    "&emsp;$=\\delta_t + \\gamma (G_{t+1} - Q(S_{t+1}, A_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77702050-b9d8-4583-b9e1-648a3311f077",
   "metadata": {},
   "source": [
    "&emsp;$= \\delta_t + \\gamma(\\delta_{t+1} + \\gamma(G_{t+2} - Q(S_{t+2}, A_{t+2})))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25998c2f-45bc-445e-9b3d-46c94b48075b",
   "metadata": {},
   "source": [
    "&emsp;$=\\delta_t + \\gamma \\delta_{t+1} + \\gamma \\delta_{t+2} +$\n",
    "$... +\\gamma^{T-t}(G_T - Q(S_t, A_t))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f18a12-683d-4b7e-99fc-223e057dc5da",
   "metadata": {},
   "source": [
    "&emsp;$\\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b449942a-c614-47d7-811a-5a20b88ce1ec",
   "metadata": {},
   "source": [
    "### 6.9\n",
    "Windy Gridworld with King’s Moves (programming) Re-solve the windy\n",
    "gridworld assuming eight possible actions, including the diagonal moves, rather than four.\n",
    "How much better can you do with the extra actions? Can you do even better by including\n",
    "a ninth action that causes no movement at all other than that caused by the wind?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f0672-0dda-4c7c-bebf-5a547a75a571",
   "metadata": {},
   "source": [
    "### 6.10\n",
    "Stochastic Wind (programming) Re-solve the windy gridworld task with\n",
    "King’s moves, assuming that the effect of the wind, if there is any, is stochastic, sometimes\n",
    "varying by 1 from the mean values given for each column. That is, a third of the time\n",
    "you move exactly according to these values, as in the previous exercise, but also a third\n",
    "of the time you move one cell above that, and another third of the time you move one\n",
    "cell below that. For example, if you are one cell to the right of the goal and you move\n",
    "`left`, then one-third of the time you move one cell above the goal, one-third of the time\n",
    "you move two cells above the goal, and one-third of the time you move to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1d4ad-ba6d-4d06-a42c-4ac35e00600c",
   "metadata": {},
   "source": [
    "### 6.11\n",
    "Why is $Q$-learning considered an *off-policy* control method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545ad7f3-158c-4cbd-bb0d-6cad43f51bad",
   "metadata": {},
   "source": [
    "It's because when we predict the value function we use greedy approach\n",
    "to see at this time which action would be the best in next action, so\n",
    "that we can estabilish at this time what we think is the best policy\n",
    "following action $A_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81d20e3-0a45-47f0-ac3b-cd566a1f4d07",
   "metadata": {},
   "source": [
    "### 6.12\n",
    "Suppose action selection is greedy. Is $Q$-learning then exactly the same\n",
    "algorithm as Sarsa? Will they make exactly the same action selections and weight\n",
    "updates?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9212dd-f48c-44ce-94f9-6be9a7637dd4",
   "metadata": {},
   "source": [
    "No because the difference is in the policy evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ec018-40bf-44c7-915f-a86d2baa1195",
   "metadata": {},
   "source": [
    "### 6.13 (hard)\n",
    "What are the update equations for Double Expected Sarsa with an\n",
    "$\\epsilon$-greedy target policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a98a5-08e9-460c-9c3a-52d02c90c73a",
   "metadata": {},
   "source": [
    "### 6.14\n",
    "Describe how the task of Jack’s Car Rental (Example 4.2) could be\n",
    "reformulated in terms of afterstates. Why, in terms of this specific task, would such a\n",
    "reformulation be likely to speed convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b74d2-f268-4d3f-9076-11eb120467a3",
   "metadata": {},
   "source": [
    "We can have same number of cars in locations from different states when we\n",
    "moved the cars to another location and later rented cars were returned or if\n",
    "we had the cars returned in both places.  \n",
    "It would speed the convergence since we wouldn't have to assest both situations\n",
    "but we can just focus on the afterstate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
