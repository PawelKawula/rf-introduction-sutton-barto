{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd52306-9868-4ff1-a001-f51802df728e",
   "metadata": {},
   "source": [
    "### 1\n",
    "The nonplanning method looks particularly poor in Figure 8.3 because it is\n",
    "a one-step method; a method using multi-step bootstrapping would do better. Do you\n",
    "think one of the multi-step bootstrapping methods from Chapter 7 could do as well as\n",
    "the Dyna method? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc855f8-e0f0-4089-8cf1-4dab9bf71cef",
   "metadata": {},
   "source": [
    "I belive that both sarsa and expected sarsa could replace this method,\n",
    "and that's because they too take into consideration couple of states and rewards\n",
    "to update it's value functions, although they may be more computiationaly more expensive\n",
    "I belive that value function would grow faster with dyna, at least for intermediate states,\n",
    "since gamma factor would be just to the power of 1 every time we use planning.\n",
    "Actually compute this:\n",
    "For n-step method:  \n",
    "&emsp;$V(s) = R_{t+1} + ... \\gamma^{n-1} R_{t+n} +\\gamma^n V(S_{t+n})$\n",
    "For Dyna:  \n",
    "&emsp;$V(s) = R_{t+1} + \\gamma {max}_s V(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f32a62c-adef-4439-9c06-ec9b6f613951",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">Look closely at the excersise, since most of the time rewards are 0 n-step predictions aren't going to work<br/>\n",
    "Only if we have a situation in which $V(S_{t+n})$ or $R_{t+x}$ for $x \\in [1, n]$ is non-zero for an update would we have any change<br/>\n",
    "We also have to consider that even if n is big enough Dyna would be more more efficient, since we would only have to deal with\n",
    "one multiplications, not ?n-1?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be70741e-07d9-4c3b-8e41-99baf377abe2",
   "metadata": {},
   "source": [
    "### 2\n",
    "Why did the Dyna agent with exploration bonus, Dyna-Q+, perform\n",
    "better in the first phase as well as in the second phase of the blocking and shortcut\n",
    "experiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d127ff4-348c-471d-b898-dce7d649e676",
   "metadata": {},
   "source": [
    "It's because of the exploration is found the optimal path quicker, and when we opened \n",
    "the second path is actually found the other better way after being forced to approach it\n",
    "from time to time and eventually updating it's values to better math current dynamics of \n",
    "the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea384e-7ab8-408f-8764-85ed4a30748e",
   "metadata": {},
   "source": [
    "Plus in the first experiment i belive that it found a better way to approach the gap as well as\n",
    "getting to the goal after, since it experimented more and so we found more attractive path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546c1edd-fc5d-431b-997c-04b8c940f7d6",
   "metadata": {},
   "source": [
    "### 3\n",
    "Careful inspection of Figure 8.5 reveals that the difference between Dyna-Q+\n",
    "and Dyna-Q narrowed slightly over the first part of the experiment. What is the reason\n",
    "for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0958871-c673-4229-bdf3-3aea7fc2c0a9",
   "metadata": {},
   "source": [
    "It's because Dyna-Q+ kept exploring still not opened path while normal Dyna just stick to\n",
    "it, so since greedy action was more optimal until we opened the path difference narrowed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86838f-ad7a-4a73-96e3-12b9c933b4e3",
   "metadata": {},
   "source": [
    "### 4 (programming)\n",
    "The exploration bonus described above actually changes\n",
    "the estimated values of states and actions. Is this necessary? Suppose the bonus $\\kappa \\sqrt{\\tau}$ \n",
    "was used not in updates, but solely in action selection.\n",
    "That is, suppose the action\n",
    "selected was always that for which $Q(S_t , a) + \\kappa \\sqrt{\\tau(S_t , a)}$ was maximal. Carry out a\n",
    "gridworld experiment that tests and illustrates the strengths and weaknesses of this\n",
    "alternate approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a3323e-ae07-4b05-a5fa-ab43c19dc8a9",
   "metadata": {},
   "source": [
    "### 5\n",
    "How might the tabular Dyna-Q algorithm shown on page 164 be modified\n",
    "to handle stochastic environments? How might this modification perform poorly on\n",
    "changing environments such as considered in this section? How could the algorithm be\n",
    "modified to handle stochastic environments and changing environments?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d59b9-6fad-4fdf-83e3-91fe373e9a26",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">it's model could return arrays containing tuples of reward and state with probabilities\n",
    "of them occuring, then it could update value functions for all of these tuples but\n",
    "change $\\alpha$ to $\\alpha \\cdot p$, where $p$ is this probability.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e217a98-ee20-412f-894a-aa0843ba8254",
   "metadata": {},
   "source": [
    "That would be very difficult to manage, since it will be computionally expensive, so maybe the best scenario would be\n",
    "to <span style=\"color:red\">use some form of state aggregation.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae821b23-0649-472c-9f11-1e0a379e2f17",
   "metadata": {},
   "source": [
    "We can decrease speed of learning for older examples, we can decrease probability of using the data the older it is,\n",
    "we can delete the oldest data an some intervals.  \n",
    "We can fe. use those techniques, then analyse variance of rewards for each state and action, and if we see small variace\n",
    "increase $\\alpha$ and use even older data than before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec99e77e-114d-4872-a88e-917cab6873d4",
   "metadata": {},
   "source": [
    "### 6\n",
    "The analysis above (8.5, page 172-174) assumed that all of the $b$ possible next states \n",
    "were\n",
    "equally likely to occur. Suppose instead that the distribution was highly skewed, that\n",
    "some of the b states were much more likely to occur than most. Would this strengthen or\n",
    "weaken the case for sample updates over expected updates? Support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e912f888-a1a7-45d1-be41-ea60aacd2489",
   "metadata": {},
   "source": [
    "It would probably lessen the difference between the two since we will have better \n",
    "estimates for states that we are likely to occur, so we wont waste computational time\n",
    "for cases which we may never even encounter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390150f3-649f-4e74-ae48-58fe50f1889c",
   "metadata": {},
   "source": [
    "Plus since the probability of the state occuring would be higher than the difference would be lessen since\n",
    "they both multiply the same update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79371b3-44f2-4039-86e2-91deb85627d8",
   "metadata": {},
   "source": [
    "### 7\n",
    "Some of the graphs in Figure 8.8 seem to be scalloped in their early portions,\n",
    "particularly the upper graph for $b = 1$ and the uniform distribution. Why do you think\n",
    "this is? What aspects of the data shown support your hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72fedd0-b4c1-494e-9ff6-d42efbabecc9",
   "metadata": {},
   "source": [
    "That's because with smaller b we have bigger variance, since we have only two possible next states\n",
    "they probably have bigger difference between them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f00cdf-3db5-4860-aa9b-a73806c474dc",
   "metadata": {},
   "source": [
    "Each sweep would bring decent changes, especially for b=1 where the change for one state is fully transferred to other states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa41421-ef7b-4e29-a7a6-53fa0c449b02",
   "metadata": {},
   "source": [
    "### 8 (programming)\n",
    "Replicate the experiment whose results are shown in the\n",
    "lower part of Figure 8.8, then try the same experiment but with $b = 3$. Discuss the\n",
    "meaning of your results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
